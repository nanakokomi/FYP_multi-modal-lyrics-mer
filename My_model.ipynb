{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error #add rmse\n",
    "from data import merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and drop VADER columns\n",
    "df_train = pd.read_csv('data/merged/merged_cleaned_sentiment_train.csv').drop(['pos','neg','neu', 'compound'], axis = 1)\n",
    "df_val = pd.read_csv('data/merged/merged_cleaned_sentiment_validation.csv').drop(['pos','neg','neu', 'compound'], axis = 1)\n",
    "df_test = pd.read_csv('data/merged/merged_cleaned_sentiment_test.csv').drop(['pos','neg','neu', 'compound'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save audio features\n",
    "df_train = df_train[['danceability', 'energy', 'instrumentalness', 'valence','mode', 'y_valence', 'y_arousal']]\n",
    "df_val = df_val[['danceability', 'energy', 'instrumentalness', 'valence','mode', 'y_valence', 'y_arousal']]\n",
    "df_test = df_test[['danceability', 'energy', 'instrumentalness', 'valence','mode','y_valence', 'y_arousal']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lyrics features and merge with audio\n",
    "df_train = pd.concat([df_train, pd.read_csv('data/lyrics/lyrics_features_train.csv').iloc[:, :-200]], axis=1)\n",
    "df_val = pd.concat([df_val, pd.read_csv('data/lyrics/lyrics_features_val.csv').iloc[:, :-200]], axis=1)\n",
    "df_test = pd.concat([df_test, pd.read_csv('data/lyrics/lyrics_features_test.csv').iloc[:, :-200]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values from the training, validation, and test datasets\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['danceability', 'energy', 'instrumentalness', 'valence', 'mode',\n",
       "       'y_valence', 'y_arousal', 'Unnamed: 0', 'pos', 'neg',\n",
       "       ...\n",
       "       'tfidf_pca_91', 'tfidf_pca_92', 'tfidf_pca_93', 'tfidf_pca_94',\n",
       "       'tfidf_pca_95', 'tfidf_pca_96', 'tfidf_pca_97', 'tfidf_pca_98',\n",
       "       'tfidf_pca_99', 'tfidf_pca_100'],\n",
       "      dtype='object', length=112)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output colums\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training set\n",
    "# X_train: Features for training set, excluding the target variables 'y_valence' and 'y_arousal'\n",
    "X_train = df_train.drop(['y_valence', 'y_arousal'], axis=1).values\n",
    "# y_train_valence: Target variable 'y_valence' for training set\n",
    "y_train_valence = df_train.y_valence.values \n",
    "# y_train_arousal: Target variable 'y_arousal' for training set\n",
    "y_train_arousal = df_train.y_arousal.values\n",
    "    \n",
    "# Validation set\n",
    "# X_val: Features for validation set, excluding the target variables 'y_valence' and 'y_arousal'\n",
    "X_val = df_val.drop(['y_valence', 'y_arousal'], axis=1).values\n",
    "# y_val_valence: Target variable 'y_valence' for validation set\n",
    "y_val_valence = df_val.y_valence.values \n",
    "# y_val_arousal: Target variable 'y_arousal' for validation set\n",
    "y_val_arousal = df_val.y_arousal.values \n",
    "\n",
    "# Test set\n",
    "# X_test: Features for test set, excluding the target variables 'y_valence' and 'y_arousal'\n",
    "X_test = df_test.drop(['y_valence', 'y_arousal'], axis=1).values\n",
    "# y_test_valence: Target variable 'y_valence' for test set\n",
    "y_test_valence = df_test.y_valence.values \n",
    "# y_test_arousal: Target variable 'y_arousal' for test set\n",
    "y_test_arousal = df_test.y_arousal.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE+R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def evaluate_model1(X_val, y_1_validation, y_2_validation, model_predictions_file='predictions.csv'):\n",
    "    \"\"\"Evaluate the trained model using different evaluation criteria\"\"\"\n",
    "    \n",
    "    # Load the saved predictions from the CSV file\n",
    "    df_predictions = pd.read_csv(model_predictions_file)\n",
    "    \n",
    "    # Get the true values from validation data\n",
    "    true_valence = y_1_validation\n",
    "    true_arousal = y_2_validation\n",
    "\n",
    "    # Evaluate RMSE for Valence\n",
    "    rmse_valence = mean_squared_error(true_valence, df_predictions['pred_valence'], squared=False)\n",
    "    rmse_arousal = mean_squared_error(true_arousal, df_predictions['pred_arousal'], squared=False)\n",
    "\n",
    "    # Evaluate R¬≤ for Valence and Arousal\n",
    "    r2_valence = r2_score(true_valence, df_predictions['pred_valence'])\n",
    "    r2_arousal = r2_score(true_arousal, df_predictions['pred_arousal'])\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"RMSE for Valence: {rmse_valence:.3f}\")\n",
    "    print(f\"RMSE for Arousal: {rmse_arousal:.3f}\")\n",
    "    \n",
    "    print(f\"R¬≤ for Valence: {r2_valence:.3f}\")\n",
    "    print(f\"R¬≤ for Arousal: {r2_arousal:.3f}\")\n",
    "    \n",
    "    # Return evaluation results as a dictionary\n",
    "    eval_results = {\n",
    "        'rmse_valence': rmse_valence,\n",
    "        'rmse_arousal': rmse_arousal,\n",
    "        'r2_valence': r2_valence,\n",
    "        'r2_arousal': r2_arousal\n",
    "    }\n",
    "\n",
    "    return eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(X_val, y_1_validation, y_2_validation, model_predictions_file='predictions.csv'):\n",
    "    \"\"\"Evaluate the trained model using different evaluation criteria, including Normalized RMSE\"\"\"\n",
    "    \n",
    "    # Load the saved predictions from the CSV file\n",
    "    df_predictions = pd.read_csv(model_predictions_file)\n",
    "    \n",
    "    # Get the true values from validation data\n",
    "    true_valence = y_1_validation\n",
    "    true_arousal = y_2_validation\n",
    "\n",
    "    # Ensure predictions are in the original range (if necessary)\n",
    "    # If predictions are standardized, use the inverse_transform of your scaler before proceeding.\n",
    "    # Example: df_predictions['pred_valence'] = scaler.inverse_transform(df_predictions[['pred_valence']])\n",
    "\n",
    "    # Compute RMSE\n",
    "    rmse_valence = mean_squared_error(true_valence, df_predictions['pred_valence'], squared=False)\n",
    "    rmse_arousal = mean_squared_error(true_arousal, df_predictions['pred_arousal'], squared=False)\n",
    "\n",
    "    # Compute Normalized RMSE\n",
    "    valence_range = max(true_valence) - min(true_valence)\n",
    "    arousal_range = max(true_arousal) - min(true_arousal)\n",
    "\n",
    "    normalized_rmse_valence = rmse_valence / valence_range if valence_range > 0 else None\n",
    "    normalized_rmse_arousal = rmse_arousal / arousal_range if arousal_range > 0 else None\n",
    "\n",
    "    # Compute R¬≤\n",
    "    r2_valence = r2_score(true_valence, df_predictions['pred_valence'])\n",
    "    r2_arousal = r2_score(true_arousal, df_predictions['pred_arousal'])\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"RMSE for Valence: {rmse_valence:.4f}\")\n",
    "    print(f\"RMSE for Arousal: {rmse_arousal:.4f}\")\n",
    "    print(f\"Normalized RMSE for Valence: {normalized_rmse_valence:.4f}\" if normalized_rmse_valence is not None else \"Valence range is zero, cannot compute NRMSE.\")\n",
    "    print(f\"Normalized RMSE for Arousal: {normalized_rmse_arousal:.4f}\" if normalized_rmse_arousal is not None else \"Arousal range is zero, cannot compute NRMSE.\")\n",
    "    print(f\"R¬≤ for Valence: {r2_valence:.4f}\")\n",
    "    print(f\"R¬≤ for Arousal: {r2_arousal:.4f}\")\n",
    "    \n",
    "    # Return evaluation results as a dictionary\n",
    "    eval_results = {\n",
    "        'rmse_valence': rmse_valence,\n",
    "        'rmse_arousal': rmse_arousal,\n",
    "        'normalized_rmse_valence': normalized_rmse_valence,\n",
    "        'normalized_rmse_arousal': normalized_rmse_arousal,\n",
    "        'r2_valence': r2_valence,\n",
    "        'r2_arousal': r2_arousal\n",
    "    }\n",
    "\n",
    "    return eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_on_test(X_test, y_test_valence, y_test_arousal, model_valence, model_arousal,scaler_X=None):\n",
    "    \"\"\"\n",
    "    Âú®ÂéüÂßãÂ∞∫Â∫¶‰∏äËØÑ‰º∞Ê®°ÂûãÊÄßËÉΩÔºöËæìÂá∫ RMSE„ÄÅNormalized RMSE Âíå R¬≤„ÄÇ\n",
    "    \"\"\"\n",
    "    # 1. ÂΩí‰∏ÄÂåñÊµãËØïÈõÜÁâπÂæÅÔºàÂ¶ÇÊûúÊèê‰æõ‰∫Ü scalerÔºâ\n",
    "    if scaler_X is not None:\n",
    "     X_test = scaler_X.transform(X_test)\n",
    "    # 1. Ê®°ÂûãÈ¢ÑÊµã\n",
    "    pred_val = model_valence.predict(X_test)\n",
    "    pred_arou = model_arousal.predict(X_test)\n",
    "\n",
    "    # 2. RMSE\n",
    "    rmse_val = mean_squared_error(y_test_valence, pred_val, squared=False)\n",
    "    rmse_arou = mean_squared_error(y_test_arousal, pred_arou, squared=False)\n",
    "\n",
    "    # 3. Normalized RMSE\n",
    "    valence_range = np.max(y_test_valence) - np.min(y_test_valence)\n",
    "    arousal_range = np.max(y_test_arousal) - np.min(y_test_arousal)\n",
    "\n",
    "    nrmse_val = rmse_val / valence_range if valence_range > 0 else None\n",
    "    nrmse_arou = rmse_arou / arousal_range if arousal_range > 0 else None\n",
    "\n",
    "    # 4. R¬≤\n",
    "    r2_val = r2_score(y_test_valence, pred_val)\n",
    "    r2_arou = r2_score(y_test_arousal, pred_arou)\n",
    "\n",
    "    # 5. ËæìÂá∫ÁªìÊûú\n",
    "    print(\"üìä [Test Set Evaluation]\")\n",
    "    print(f\"RMSE (Valence): {rmse_val:.4f}\")\n",
    "    print(f\"RMSE (Arousal): {rmse_arou:.4f}\")\n",
    "    print(f\"Normalized RMSE (Valence): {nrmse_val:.4f}\" if nrmse_val is not None else \"Valence range is zero, cannot compute NRMSE.\")\n",
    "    print(f\"Normalized RMSE (Arousal): {nrmse_arou:.4f}\" if nrmse_arou is not None else \"Arousal range is zero, cannot compute NRMSE.\")\n",
    "    print(f\"R¬≤ (Valence): {r2_val:.4f}\")\n",
    "    print(f\"R¬≤ (Arousal): {r2_arou:.4f}\")\n",
    "\n",
    "    # 6. ËøîÂõûÂèØÈÄâÁªìÊûúÂ≠óÂÖ∏\n",
    "    return {\n",
    "        'rmse_valence': rmse_val,\n",
    "        'rmse_arousal': rmse_arou,\n",
    "        'normalized_rmse_valence': nrmse_val,\n",
    "        'normalized_rmse_arousal': nrmse_arou,\n",
    "        'r2_valence': r2_val,\n",
    "        'r2_arousal': r2_arou\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL:LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def train_regression(X, y_1, y_2, X_val, param_grid=None):\n",
    "    \"\"\" Train the regression model with GridSearchCV \"\"\"\n",
    "    \n",
    "    # Default parameters for grid search\n",
    "    if param_grid is None:\n",
    "        param_grid = {'fit_intercept': [True, False], 'positive': [True, False]}\n",
    "    \n",
    "    # Initialize models for both targets\n",
    "    lr_val = LinearRegression()\n",
    "    lr_arou = LinearRegression()\n",
    "    \n",
    "    # Grid search for both models\n",
    "    clf_vale = GridSearchCV(lr_val, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "    \n",
    "    clf_arou = GridSearchCV(lr_arou, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Fit models to training data\n",
    "    clf_vale.fit(X, y_1)\n",
    "    clf_arou.fit(X, y_2)\n",
    "    \n",
    "    # Print best results on training data\n",
    "    print()\n",
    "    print(f\"Best parameter for Valence (CV score={clf_vale.best_score_:.3f}):\")\n",
    "    print(clf_vale.best_params_)\n",
    "    \n",
    "    print()\n",
    "    print(f\"Best parameter for Arousal (CV score={clf_arou.best_score_:.3f}):\")\n",
    "    print(clf_arou.best_params_)\n",
    "\n",
    "    # Initialize models with best parameters\n",
    "    lr_val_top = LinearRegression(fit_intercept=clf_vale.best_params_['fit_intercept'],  \n",
    "                                  positive=clf_vale.best_params_['positive'])\n",
    "    lr_arou_top = LinearRegression(fit_intercept=clf_arou.best_params_['fit_intercept'],  \n",
    "                                   positive=clf_arou.best_params_['positive'])\n",
    "\n",
    "    # Fit optimal models to the entire training data\n",
    "    lr_val_top.fit(X, y_1)\n",
    "    lr_arou_top.fit(X, y_2)\n",
    "\n",
    "    # Predict for validation data\n",
    "    predictions_val = lr_val_top.predict(X_val)  # Predictions for Valence\n",
    "    predictions_arou = lr_arou_top.predict(X_val)  # Predictions for Arousal\n",
    "\n",
    "    # Save predictions for validation data to CSV\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'pred_valence': predictions_val,  # Valence predictions\n",
    "        'pred_arousal': predictions_arou  # Arousal predictions\n",
    "    })\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    df_predictions.to_csv('predictions2.csv', index=False)\n",
    "\n",
    "    print(\"‚úÖ Training completed and predictions saved!\")\n",
    "    return lr_val_top, lr_arou_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def train_regression(X, y_1, y_2, X_val, track_ids, param_grid=None):\n",
    "    \"\"\" Train the regression model with GridSearchCV and save predictions with track IDs \"\"\"\n",
    "\n",
    "    # Default parameters for grid search\n",
    "    if param_grid is None:\n",
    "        param_grid = {'fit_intercept': [True, False], 'positive': [True, False]}\n",
    "    \n",
    "    # Initialize models for both targets\n",
    "    lr_val = LinearRegression()\n",
    "    lr_arou = LinearRegression()\n",
    "    \n",
    "    # Grid search for both models\n",
    "    clf_vale = GridSearchCV(lr_val, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "    \n",
    "    clf_arou = GridSearchCV(lr_arou, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Fit models to training data\n",
    "    clf_vale.fit(X, y_1)\n",
    "    clf_arou.fit(X, y_2)\n",
    "    \n",
    "    # Print best results on training data\n",
    "    print()\n",
    "    print(f\"Best parameter for Valence (CV score={clf_vale.best_score_:.3f}):\")\n",
    "    print(clf_vale.best_params_)\n",
    "    \n",
    "    print()\n",
    "    print(f\"Best parameter for Arousal (CV score={clf_arou.best_score_:.3f}):\")\n",
    "    print(clf_arou.best_params_)\n",
    "\n",
    "    # Initialize models with best parameters\n",
    "    lr_val_top = LinearRegression(fit_intercept=clf_vale.best_params_['fit_intercept'],  \n",
    "                                  positive=clf_vale.best_params_['positive'])\n",
    "    lr_arou_top = LinearRegression(fit_intercept=clf_arou.best_params_['fit_intercept'],  \n",
    "                                   positive=clf_arou.best_params_['positive'])\n",
    "\n",
    "    # Fit optimal models to the entire training data\n",
    "    lr_val_top.fit(X, y_1)\n",
    "    lr_arou_top.fit(X, y_2)\n",
    "\n",
    "    # Predict for validation data\n",
    "    predictions_val = lr_val_top.predict(X_val)  # Predictions for Valence\n",
    "    predictions_arou = lr_arou_top.predict(X_val)  # Predictions for Arousal\n",
    "\n",
    "    # Ensure track_ids length matches predictions length\n",
    "    if len(track_ids) != len(predictions_val):\n",
    "        raise ValueError(\"Mismatch: The number of track IDs does not match the number of predictions!\")\n",
    "\n",
    "    # Save predictions for validation data to CSV with track ID\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'track_id': track_ids,  # Track ID column\n",
    "        'pred_valence': predictions_val,  # Valence predictions\n",
    "        'pred_arousal': predictions_arou  # Arousal predictions\n",
    "    })\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    df_predictions.to_csv('predictions1.csv', index=False)\n",
    "\n",
    "    print(\"‚úÖ Training completed and predictions saved with Track IDs!\")\n",
    "    return lr_val_top, lr_arou_top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Best parameter for Valence (CV score=-0.912):\n",
      "{'fit_intercept': True, 'positive': False}\n",
      "\n",
      "Best parameter for Arousal (CV score=-0.778):\n",
      "{'fit_intercept': False, 'positive': False}\n",
      "‚úÖ Training completed and predictions saved with Track IDs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearRegression(), LinearRegression(fit_intercept=False))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ËØªÂèñÊï∞ÊçÆ\n",
    "df = pd.read_csv('data/merged/merged_cleaned_sentiment_validation.csv')\n",
    "\n",
    "# ÂÅáËÆæÈ™åËØÅÈõÜÊòØ‰ªéÂéüÂßãÊï∞ÊçÆÈõÜ‰∏≠ÈÄâÂá∫ÁöÑ\n",
    "df_validation = df.iloc[:len(X_val)]  # Âè™‰øùÁïôÈ™åËØÅÈõÜÈÉ®ÂàÜ\n",
    "\n",
    "# ÊèêÂèñÈ™åËØÅÈõÜÂØπÂ∫îÁöÑ Track ID\n",
    "track_ids = df_validation['trackname'].values  # ÊàñËÄÖ df_validation['track_id'] ÂèñÂîØ‰∏ÄÊ†áËØÜÁ¨¶\n",
    "\n",
    "# ËÆ≠ÁªÉÂõûÂΩíÊ®°ÂûãÔºåÂπ∂Â≠òÂÇ®È¢ÑÊµãÁªìÊûú\n",
    "train_regression(X_train, y_train_valence, y_train_arousal, X_val, track_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Best parameter for Valence (CV score=-0.912):\n",
      "{'fit_intercept': True, 'positive': False}\n",
      "\n",
      "Best parameter for Arousal (CV score=-0.778):\n",
      "{'fit_intercept': False, 'positive': False}\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'predictions2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming X_train, y_train, X_validation, y_validation are your data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lr_val_model, lr_arou_model \u001b[38;5;241m=\u001b[39m train_regression(X_train, y_train_valence, y_train_arousal, X_val)\n",
      "Cell \u001b[1;32mIn[67], line 65\u001b[0m, in \u001b[0;36mtrain_regression\u001b[1;34m(X, y_1, y_2, X_val, param_grid)\u001b[0m\n\u001b[0;32m     59\u001b[0m df_predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_valence\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions_val,  \u001b[38;5;66;03m# Valence predictions\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_arousal\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions_arou  \u001b[38;5;66;03m# Arousal predictions\u001b[39;00m\n\u001b[0;32m     62\u001b[0m })\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Save predictions to a CSV file\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m df_predictions\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Training completed and predictions saved!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lr_val_top, lr_arou_top\n",
      "File \u001b[1;32mc:\\Users\\Alice\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3903\u001b[0m     path_or_buf,\n\u001b[0;32m   3904\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3905\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3906\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3907\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3908\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3909\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3910\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3911\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3912\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3913\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3914\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3915\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3916\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3917\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3918\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3919\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Alice\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\Alice\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    251\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    252\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    253\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    254\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\Alice\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'predictions2.csv'"
     ]
    }
   ],
   "source": [
    "# Assuming X_train, y_train, X_validation, y_validation are your data\n",
    "lr_val_model, lr_arou_model = train_regression(X_train, y_train_valence, y_train_arousal, X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2+RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence: 0.9352\n",
      "RMSE for Arousal: 0.8537\n",
      "Normalized RMSE for Valence: 0.2686\n",
      "Normalized RMSE for Arousal: 0.1678\n",
      "R¬≤ for Valence: 0.1860\n",
      "R¬≤ for Arousal: 0.1939\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_val, y_valence_val, y_arousal_val are your validation data\n",
    "eval_results = evaluate_model(X_val, y_val_valence, y_val_arousal ,model_predictions_file='predictions2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE for Valence: 0.935\n",
    "RMSE for Arousal: 0.854\n",
    "R¬≤ for Valence: 0.186\n",
    "R¬≤ for Arousal: 0.194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2558, 2)\n"
     ]
    }
   ],
   "source": [
    "df_predictions = pd.read_csv('predictions1.csv')\n",
    "print(df_predictions.shape)  # ËæìÂá∫È¢ÑÊµãÁªìÊûúÁöÑË°åÊï∞ÔºåÁ°Æ‰øù‰∏éÈ™åËØÅÈõÜ‰∏ÄËá¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2558 2558 2558\n"
     ]
    }
   ],
   "source": [
    "print(len(df_predictions), len(y_val_valence), len(y_val_arousal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä [Test Set Evaluation]\n",
      "RMSE (Valence): 0.9211\n",
      "RMSE (Arousal): 0.8569\n",
      "Normalized RMSE (Valence): 0.2493\n",
      "Normalized RMSE (Arousal): 0.1684\n",
      "R¬≤ (Valence): 0.2352\n",
      "R¬≤ (Arousal): 0.1898\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_on_test(\n",
    "    X_test,\n",
    "    y_test_valence,\n",
    "    y_test_arousal,\n",
    "    lr_val_model,\n",
    "    lr_arou_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call func MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def train_mlp(X, y_1, y_2, X_val, param_grid=None):\n",
    "    \"\"\" Train the MLP model with GridSearchCV and save the predictions \"\"\"\n",
    "\n",
    "    # Normalize the features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Default parameters for grid search\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(5,), (10,), (15,), (5,5), (10,10), (15,15), (5,5,5), (10,10,10), (15,15,15)], \n",
    "            'max_iter': [500, 1000, 2000, 2500]\n",
    "        }\n",
    "\n",
    "    # Initialize models for both targets\n",
    "    mlp_val = MLPRegressor(random_state=2)\n",
    "    mlp_arou = MLPRegressor(random_state=2)\n",
    "    \n",
    "    # Grid search for Valence\n",
    "    clf_vale = GridSearchCV(mlp_val, param_grid, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1, return_train_score=True)\n",
    "    clf_vale.fit(X, y_1)\n",
    "    print(f\"Best parameter for Valence (CV score={-clf_vale.best_score_:.3f}): {clf_vale.best_params_}\")\n",
    "    \n",
    "    # Initialize model with best parameters and fit\n",
    "    mlp_val_top = MLPRegressor(hidden_layer_sizes=clf_vale.best_params_['hidden_layer_sizes'],\n",
    "                               max_iter=clf_vale.best_params_['max_iter'],\n",
    "                               random_state=2)\n",
    "    mlp_val_top.fit(X, y_1)\n",
    "\n",
    "    # Grid search for Arousal\n",
    "    clf_arou = GridSearchCV(mlp_arou, param_grid, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1, return_train_score=True)\n",
    "    clf_arou.fit(X, y_2)\n",
    "    print(f\"Best parameter for Arousal (CV score={-clf_arou.best_score_:.3f}): {clf_arou.best_params_}\")\n",
    "    \n",
    "    # Initialize model with best parameters and fit\n",
    "    mlp_arou_top = MLPRegressor(hidden_layer_sizes=clf_arou.best_params_['hidden_layer_sizes'],\n",
    "                                max_iter=clf_arou.best_params_['max_iter'],\n",
    "                                random_state=2)\n",
    "    mlp_arou_top.fit(X, y_2)\n",
    "\n",
    "    # Predict for validation data\n",
    "    predictions_val = mlp_val_top.predict(X_val)  # Predictions for Valence\n",
    "    predictions_arou = mlp_arou_top.predict(X_val)  # Predictions for Arousal\n",
    "\n",
    "    # Save predictions for validation data to CSV\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'pred_valence': predictions_val,  # Valence predictions\n",
    "        'pred_arousal': predictions_arou  # Arousal predictions\n",
    "    })\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    df_predictions.to_csv('predictions_mlp.csv', index=False)\n",
    "\n",
    "    print(\"‚úÖ MLP training completed and predictions saved!\")\n",
    "    return mlp_val_top, mlp_arou_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameter for Valence (CV score=0.903): {'hidden_layer_sizes': (5, 5), 'max_iter': 500}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameter for Arousal (CV score=0.769): {'hidden_layer_sizes': (5, 5, 5), 'max_iter': 500}\n",
      "‚úÖ MLP training completed and predictions saved!\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train, y_train_valence, y_train_arousal, X_val are available\n",
    "mlp_val_model, mlp_arou_model = train_mlp(X_train, y_train_valence, y_train_arousal, X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence: 0.9324\n",
      "RMSE for Arousal: 0.8468\n",
      "Normalized RMSE for Valence: 0.2678\n",
      "Normalized RMSE for Arousal: 0.1664\n",
      "R¬≤ for Valence: 0.1909\n",
      "R¬≤ for Arousal: 0.2069\n"
     ]
    }
   ],
   "source": [
    "eval_results = evaluate_model(X_val, y_val_valence, y_val_arousal, model_predictions_file='predictions_mlp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23934316834217395\n",
      "0.20374419332813887\n"
     ]
    }
   ],
   "source": [
    "### Test Set - MLP \n",
    "mlp_val = MLPRegressor(hidden_layer_sizes=mlp_val_model.hidden_layer_sizes, max_iter=mlp_val_model.max_iter, random_state=2)\n",
    "mlp_arou = MLPRegressor(hidden_layer_sizes=mlp_arou_model.hidden_layer_sizes, max_iter=mlp_arou_model.max_iter, random_state=2)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled.shape\n",
    "print(mlp_val.fit(X_train_scaled, y_train_valence).score(X_test_scaled, y_test_valence))\n",
    "print(mlp_arou.fit(X_train_scaled, y_train_arousal).score(X_test_scaled, y_test_arousal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä [Test Set Evaluation]\n",
      "RMSE (Valence): 0.9186\n",
      "RMSE (Arousal): 0.8495\n",
      "Normalized RMSE (Valence): 0.2486\n",
      "Normalized RMSE (Arousal): 0.1669\n",
      "R¬≤ (Valence): 0.2393\n",
      "R¬≤ (Arousal): 0.2037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ËÆ≠ÁªÉÊó∂ÂÅöÁöÑÂΩí‰∏ÄÂåñ\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "mlp_val_model.fit(X_train_scaled, y_train_valence)\n",
    "mlp_arou_model.fit(X_train_scaled, y_train_arousal)\n",
    "\n",
    "# Âú®ËØÑ‰º∞ÂáΩÊï∞ÈáåÁõ¥Êé•‰º† scaler\n",
    "results = evaluate_on_test(\n",
    "    X_test,\n",
    "    y_test_valence,\n",
    "    y_test_arousal,\n",
    "    mlp_val_model,\n",
    "    mlp_arou_model,\n",
    "    scaler_X=scaler  # ‚úÖ ‰º†ÂÖ•Áî®‰∫éËÆ≠ÁªÉÁöÑ scaler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Def Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def train_rf(X, y_1, y_2, X_val):\n",
    "    \"\"\" Train Random Forest model and save predictions for validation data \"\"\"\n",
    "\n",
    "    # Initialize models for Valence and Arousal\n",
    "    rf_val = RandomForestRegressor(random_state=0)\n",
    "    rf_arou = RandomForestRegressor(random_state=0)\n",
    "\n",
    "    # Hyperparameter grid for GridSearchCV\n",
    "    param_grid = { \n",
    "        'n_estimators': [100, 500],\n",
    "        'max_depth': [5, 10, 15]\n",
    "    }\n",
    "\n",
    "    # Grid search for Valence model\n",
    "    clf_vale = GridSearchCV(rf_val, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Grid search for Arousal model\n",
    "    clf_arou = GridSearchCV(rf_arou, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Fit models for Valence and Arousal\n",
    "    clf_vale.fit(X, y_1)\n",
    "    clf_arou.fit(X, y_2)\n",
    "\n",
    "    # Print best parameters found by GridSearchCV\n",
    "    print(f\"Best parameter for Valence (CV score={-clf_vale.best_score_:.3f}):\")\n",
    "    print(clf_vale.best_params_)\n",
    "\n",
    "    print(f\"Best parameter for Arousal (CV score={-clf_arou.best_score_:.3f}):\")\n",
    "    print(clf_arou.best_params_)\n",
    "\n",
    "    # Initialize models with best parameters\n",
    "    rf_val_best = RandomForestRegressor(n_estimators=clf_vale.best_params_['n_estimators'],\n",
    "                                        max_depth=clf_vale.best_params_['max_depth'],\n",
    "                                        random_state=0)\n",
    "    \n",
    "    rf_arou_best = RandomForestRegressor(n_estimators=clf_arou.best_params_['n_estimators'],\n",
    "                                         max_depth=clf_arou.best_params_['max_depth'],\n",
    "                                         random_state=0)\n",
    "\n",
    "    # Fit models with best parameters\n",
    "    rf_val_best.fit(X, y_1)\n",
    "    rf_arou_best.fit(X, y_2)\n",
    "\n",
    "    # Predict for validation data\n",
    "    predictions_val = rf_val_best.predict(X_val)  # Predictions for Valence\n",
    "    predictions_arou = rf_arou_best.predict(X_val)  # Predictions for Arousal\n",
    "\n",
    "    # Save predictions to a DataFrame\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'pred_valence': predictions_val,  # Valence predictions\n",
    "        'pred_arousal': predictions_arou  # Arousal predictions\n",
    "    })\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    df_predictions.to_csv('predictions_rf.csv', index=False)\n",
    "\n",
    "    print(\"‚úÖ Random Forest training completed and predictions saved!\")\n",
    "\n",
    "    return rf_val_best, rf_arou_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameter for Valence (CV score=0.924):\n",
      "{'max_depth': 10, 'n_estimators': 500}\n",
      "Best parameter for Arousal (CV score=0.751):\n",
      "{'max_depth': 10, 'n_estimators': 500}\n",
      "‚úÖ Random Forest training completed and predictions saved!\n"
     ]
    }
   ],
   "source": [
    "rf_val_model, rf_arou_model = train_rf(X_train, y_train_valence, y_train_arousal, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation fro Rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence: 0.9443\n",
      "RMSE for Arousal: 0.8342\n",
      "Normalized RMSE for Valence: 0.2712\n",
      "Normalized RMSE for Arousal: 0.1639\n",
      "R¬≤ for Valence: 0.1700\n",
      "R¬≤ for Arousal: 0.2304\n"
     ]
    }
   ],
   "source": [
    "eval_results = evaluate_model(X_val, y_val_valence, y_val_arousal, model_predictions_file='predictions_rf.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21563447935080082\n",
      "0.2031321009223268\n"
     ]
    }
   ],
   "source": [
    "#### Test Set - RF\n",
    "rf_val = RandomForestRegressor(n_estimators = 100, \n",
    "                                        max_depth = 5, random_state=0)\n",
    "rf_arou = RandomForestRegressor(n_estimators = 100, \n",
    "                                        max_depth = 5, random_state=0)\n",
    "\n",
    "print(rf_val.fit(X_train, y_train_valence).score(X_test, y_test_valence))\n",
    "print(rf_arou.fit(X_train, y_train_arousal).score(X_test, y_test_arousal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä [Test Set Evaluation]\n",
      "RMSE (Valence): 0.9280\n",
      "RMSE (Arousal): 0.8478\n",
      "Normalized RMSE (Valence): 0.2512\n",
      "Normalized RMSE (Arousal): 0.1666\n",
      "R¬≤ (Valence): 0.2238\n",
      "R¬≤ (Arousal): 0.2069\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ÊµãËØïÈõÜÁâπÂæÅÂΩí‰∏ÄÂåñ‰ºöÂú®ÂáΩÊï∞ÈáåËá™Âä®ÂÆåÊàêÔºàÈÄöËøá‰º†ÂÖ• scalerÔºâ\n",
    "results_rf = evaluate_on_test(\n",
    "    X_test,\n",
    "    y_test_valence,\n",
    "    y_test_arousal,\n",
    "    rf_val_model,\n",
    "    rf_arou_model,\n",
    "    scaler_X=None  # ‚úÖ ÂΩí‰∏ÄÂåñÊµãËØïÈõÜÁâπÂæÅ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def train_svr_linear(X, y_1, y_2, X_val):\n",
    "    \"\"\" Train SVR model (linear kernel) with GridSearch over C, and save predictions for validation data \"\"\"\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Define base model\n",
    "    base_svr = SVR(kernel='linear')\n",
    "\n",
    "    # Hyperparameter grid (only for C)\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 5, 10, 100]\n",
    "    }\n",
    "\n",
    "    # Grid search for Valence model\n",
    "    clf_vale = GridSearchCV(base_svr, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Grid search for Arousal model\n",
    "    clf_arou = GridSearchCV(base_svr, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Fit models\n",
    "    clf_vale.fit(X_scaled, y_1)\n",
    "    clf_arou.fit(X_scaled, y_2)\n",
    "\n",
    "    # Print best parameters\n",
    "    print(f\"Best C for Valence (CV score={-clf_vale.best_score_:.3f}): {clf_vale.best_params_['C']}\")\n",
    "    print(f\"Best C for Arousal (CV score={-clf_arou.best_score_:.3f}): {clf_arou.best_params_['C']}\")\n",
    "\n",
    "    # Use best estimators to predict\n",
    "    svr_val_best = clf_vale.best_estimator_\n",
    "    svr_arou_best = clf_arou.best_estimator_\n",
    "\n",
    "    predictions_val = svr_val_best.predict(X_val_scaled)\n",
    "    predictions_arou = svr_arou_best.predict(X_val_scaled)\n",
    "\n",
    "    # Save predictions\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'pred_valence': predictions_val,\n",
    "        'pred_arousal': predictions_arou\n",
    "    })\n",
    "\n",
    "    df_predictions.to_csv('predictions_svr_linear_gridC.csv', index=False)\n",
    "\n",
    "    print(\"‚úÖ SVR (linear kernel with GridSearch on C) training completed and predictions saved!\")\n",
    "\n",
    "    return svr_val_best, svr_arou_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best C for Valence (CV score=0.931): 0.1\n",
      "Best C for Arousal (CV score=0.780): 0.1\n",
      "‚úÖ SVR (linear kernel with GridSearch on C) training completed and predictions saved!\n"
     ]
    }
   ],
   "source": [
    "svr_val_model, svr_arou_model = train_svr_linear(X_train, y_train_valence, y_train_arousal, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence: 0.9415\n",
      "RMSE for Arousal: 0.8543\n",
      "Normalized RMSE for Valence: 0.2704\n",
      "Normalized RMSE for Arousal: 0.1679\n",
      "R¬≤ for Valence: 0.1749\n",
      "R¬≤ for Arousal: 0.1927\n"
     ]
    }
   ],
   "source": [
    "eval_results = evaluate_model(X_val, y_val_valence, y_val_arousal, model_predictions_file='predictions_svr_linear_gridC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23012613432357842\n",
      "0.18684068102087226\n"
     ]
    }
   ],
   "source": [
    "#### Test Score - SVR\n",
    "svr_val = SVR(kernel=svr_val_model.kernel, C=svr_val_model.C)\n",
    "svr_arou = SVR(kernel=svr_arou_model.kernel, C=svr_arou_model.C)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(svr_val.fit(X_train, y_train_valence).score(X_test, y_test_valence))\n",
    "print(svr_arou.fit(X_train, y_train_arousal).score(X_test, y_test_arousal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä [Test Set Evaluation]\n",
      "RMSE (Valence): 0.9242\n",
      "RMSE (Arousal): 0.8585\n",
      "Normalized RMSE (Valence): 0.2501\n",
      "Normalized RMSE (Arousal): 0.1687\n",
      "R¬≤ (Valence): 0.2301\n",
      "R¬≤ (Arousal): 0.1868\n"
     ]
    }
   ],
   "source": [
    "results_svr = evaluate_on_test(\n",
    "    X_test,\n",
    "    y_test_valence,\n",
    "    y_test_arousal,\n",
    "    svr_val_model,\n",
    "    svr_arou_model,\n",
    "    scaler_X=scaler  # ‚úÖ ‰º†ÂÖ•Áî®‰∫éËÆ≠ÁªÉÁöÑ scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match True and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           track_id  pred_valence  pred_arousal  y_valence  y_arousal\n",
      "0         Strangler      0.122002      0.006594  -1.176640  -0.314720\n",
      "1        The Letter      0.300168      0.070162  -0.780962  -0.789480\n",
      "2   Big Sky Country      0.542147      0.138120  -0.767318   0.911361\n",
      "3    Baby's Romance     -0.467359     -0.365043  -1.487725  -0.360813\n",
      "4  Over The Rainbow     -0.292903     -0.176600   1.071901   0.846830\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ËØªÂèñ Validation Êï∞ÊçÆÔºàÁúüÂÆûÂÄºÔºâ\n",
    "df_val = pd.read_csv('data/merged/merged_cleaned_sentiment_validation.csv')  # Á°Æ‰øù‰ΩøÁî®È™åËØÅÈõÜÊñá‰ª∂\n",
    "\n",
    "# ËØªÂèñÈ¢ÑÊµãÊï∞ÊçÆ\n",
    "df_predictions = pd.read_csv('predictions1.csv')\n",
    "\n",
    "# Áõ¥Êé•ÂåπÈÖçÁúüÂÆûÁöÑ Valence Âíå ArousalÔºàÂÅáËÆæÊï∞ÊçÆË°åÈ°∫Â∫èÁõ∏ÂêåÔºâ\n",
    "df_predictions['y_valence'] = df_val['y_valence'].values[:len(df_predictions)]\n",
    "df_predictions['y_arousal'] = df_val['y_arousal'].values[:len(df_predictions)]\n",
    "\n",
    "# Êü•ÁúãÂâçÂá†Ë°åÔºåÁ°Æ‰øùÂåπÈÖçÊ≠£Á°Æ\n",
    "print(df_predictions.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normaliation T+P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           track_id  y_valence_scaled  pred_valence_scaled  y_arousal_scaled  \\\n",
      "0         Strangler          0.217868             0.564546          0.396739   \n",
      "1        The Letter          0.331505             0.623268          0.303442   \n",
      "2   Big Sky Country          0.335423             0.703022          0.637681   \n",
      "3    Baby's Romance          0.128527             0.370297          0.387681   \n",
      "4  Over The Rainbow          0.863636             0.427796          0.625000   \n",
      "\n",
      "   pred_arousal_scaled  \n",
      "0             0.460284  \n",
      "1             0.488848  \n",
      "2             0.519384  \n",
      "3             0.293290  \n",
      "4             0.377966  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ‰∏∫ Valence Âíå Arousal ÁúüÂÆûÂÄºÂàõÂª∫Áã¨Á´ãÁöÑÂΩí‰∏ÄÂåñÂô®\n",
    "scaler_valence = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_arousal = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# ÂΩí‰∏ÄÂåñÁúüÂÆûÂÄº\n",
    "df_predictions['y_valence_scaled'] = scaler_valence.fit_transform(df_predictions[['y_valence']])\n",
    "df_predictions['y_arousal_scaled'] = scaler_arousal.fit_transform(df_predictions[['y_arousal']])\n",
    "\n",
    "# ‰∏∫È¢ÑÊµãÂÄºÂàõÂª∫Áã¨Á´ãÁöÑÂΩí‰∏ÄÂåñÂô®\n",
    "scaler_pred_valence = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_pred_arousal = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# ÂΩí‰∏ÄÂåñÈ¢ÑÊµãÂÄº\n",
    "df_predictions['pred_valence_scaled'] = scaler_pred_valence.fit_transform(df_predictions[['pred_valence']])\n",
    "df_predictions['pred_arousal_scaled'] = scaler_pred_arousal.fit_transform(df_predictions[['pred_arousal']])\n",
    "\n",
    "# Êü•ÁúãÂΩí‰∏ÄÂåñÂêéÁöÑÊï∞ÊçÆ\n",
    "print(df_predictions[['track_id', 'y_valence_scaled', 'pred_valence_scaled', 'y_arousal_scaled', 'pred_arousal_scaled']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_valence min: -1.93524985679, max: 1.54671444505\n",
      "pred_valence min: -1.5908582342796578, max: 1.4431918286540686\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_valence min: {df_predictions['y_valence'].min()}, max: {df_predictions['y_valence'].max()}\")\n",
    "print(f\"pred_valence min: {df_predictions['pred_valence'].min()}, max: {df_predictions['pred_valence'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence (normalized scale): 0.274\n",
      "RMSE for Arousal (normalized scale): 0.204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "rmse_valence = np.sqrt(mean_squared_error(df_predictions['y_valence_scaled'], df_predictions['pred_valence_scaled']))\n",
    "rmse_arousal = np.sqrt(mean_squared_error(df_predictions['y_arousal_scaled'], df_predictions['pred_arousal_scaled']))\n",
    "\n",
    "print(f\"RMSE for Valence (normalized scale): {rmse_valence:.3f}\")\n",
    "print(f\"RMSE for Arousal (normalized scale): {rmse_arousal:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
