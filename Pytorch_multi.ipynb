{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the torch package\n",
    "#%pip install torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from data import merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lOAD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and drop VADER columns\n",
    "df_train = pd.read_csv('data/merged/merged_cleaned_sentiment_train.csv').drop(['pos','neg','neu', 'compound'], axis = 1)\n",
    "df_val = pd.read_csv('data/merged/merged_cleaned_sentiment_validation.csv').drop(['pos','neg','neu', 'compound'], axis = 1)\n",
    "df_test = pd.read_csv('data/merged/merged_cleaned_sentiment_test.csv').drop(['pos','neg','neu', 'compound'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save audio features\n",
    "df_train = df_train[['danceability', 'energy', 'instrumentalness', 'valence','mode', 'y_valence', 'y_arousal']]\n",
    "df_val = df_val[['danceability', 'energy', 'instrumentalness', 'valence','mode', 'y_valence', 'y_arousal']]\n",
    "df_test = df_test[['danceability', 'energy', 'instrumentalness', 'valence','mode','y_valence', 'y_arousal']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lyrics features and merge with audio\n",
    "df_train = pd.concat([df_train, pd.read_csv('data/lyrics/lyrics_features_train.csv').iloc[:, :-200]], axis=1)\n",
    "df_val = pd.concat([df_val, pd.read_csv('data/lyrics/lyrics_features_val.csv').iloc[:, :-200]], axis=1)\n",
    "df_test = pd.concat([df_test, pd.read_csv('data/lyrics/lyrics_features_test.csv').iloc[:, :-200]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values from the training, validation, and test datasets\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['danceability', 'energy', 'instrumentalness', 'valence', 'mode',\n",
       "       'y_valence', 'y_arousal', 'Unnamed: 0', 'pos', 'neg',\n",
       "       ...\n",
       "       'tfidf_pca_91', 'tfidf_pca_92', 'tfidf_pca_93', 'tfidf_pca_94',\n",
       "       'tfidf_pca_95', 'tfidf_pca_96', 'tfidf_pca_97', 'tfidf_pca_98',\n",
       "       'tfidf_pca_99', 'tfidf_pca_100'],\n",
       "      dtype='object', length=112)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output colums\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training set\n",
    "# X_train: Features for training set, excluding the target variables 'y_valence' and 'y_arousal'\n",
    "X_train = df_train.drop(['y_valence', 'y_arousal'], axis=1).values\n",
    "# y_train_valence: Target variable 'y_valence' for training set\n",
    "y_train_valence = df_train.y_valence.values \n",
    "# y_train_arousal: Target variable 'y_arousal' for training set\n",
    "y_train_arousal = df_train.y_arousal.values\n",
    "    \n",
    "# Validation set\n",
    "# X_val: Features for validation set, excluding the target variables 'y_valence' and 'y_arousal'\n",
    "X_val = df_val.drop(['y_valence', 'y_arousal'], axis=1).values\n",
    "# y_val_valence: Target variable 'y_valence' for validation set\n",
    "y_val_valence = df_val.y_valence.values \n",
    "# y_val_arousal: Target variable 'y_arousal' for validation set\n",
    "y_val_arousal = df_val.y_arousal.values \n",
    "\n",
    "# Test set\n",
    "# X_test: Features for test set, excluding the target variables 'y_valence' and 'y_arousal'\n",
    "X_test = df_test.drop(['y_valence', 'y_arousal'], axis=1).values\n",
    "# y_test_valence: Target variable 'y_valence' for test set\n",
    "y_test_valence = df_test.y_valence.values \n",
    "# y_test_arousal: Target variable 'y_arousal' for test set\n",
    "y_test_arousal = df_test.y_arousal.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(X_val, y_1_validation, y_2_validation, model_predictions_file='predictions.csv'):\n",
    "    \"\"\"Evaluate the trained model using different evaluation criteria, including Normalized RMSE\"\"\"\n",
    "    \n",
    "    # Load the saved predictions from the CSV file\n",
    "    df_predictions = pd.read_csv(model_predictions_file)\n",
    "    \n",
    "    # Get the true values from validation data\n",
    "    true_valence = y_1_validation\n",
    "    true_arousal = y_2_validation\n",
    "\n",
    "    # Ensure predictions are in the original range (if necessary)\n",
    "    # If predictions are standardized, use the inverse_transform of your scaler before proceeding.\n",
    "    # Example: df_predictions['pred_valence'] = scaler.inverse_transform(df_predictions[['pred_valence']])\n",
    "\n",
    "    # Compute RMSE\n",
    "    rmse_valence = mean_squared_error(true_valence, df_predictions['pred_valence'], squared=False)\n",
    "    rmse_arousal = mean_squared_error(true_arousal, df_predictions['pred_arousal'], squared=False)\n",
    "\n",
    "    # Compute Normalized RMSE\n",
    "    valence_range = max(true_valence) - min(true_valence)\n",
    "    arousal_range = max(true_arousal) - min(true_arousal)\n",
    "\n",
    "    normalized_rmse_valence = rmse_valence / valence_range if valence_range > 0 else None\n",
    "    normalized_rmse_arousal = rmse_arousal / arousal_range if arousal_range > 0 else None\n",
    "\n",
    "    # Compute R¬≤\n",
    "    r2_valence = r2_score(true_valence, df_predictions['pred_valence'])\n",
    "    r2_arousal = r2_score(true_arousal, df_predictions['pred_arousal'])\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"RMSE for Valence: {rmse_valence:.4f}\")\n",
    "    print(f\"RMSE for Arousal: {rmse_arousal:.4f}\")\n",
    "    print(f\"Normalized RMSE for Valence: {normalized_rmse_valence:.4f}\" if normalized_rmse_valence is not None else \"Valence range is zero, cannot compute NRMSE.\")\n",
    "    print(f\"Normalized RMSE for Arousal: {normalized_rmse_arousal:.4f}\" if normalized_rmse_arousal is not None else \"Arousal range is zero, cannot compute NRMSE.\")\n",
    "    print(f\"R¬≤ for Valence: {r2_valence:.4f}\")\n",
    "    print(f\"R¬≤ for Arousal: {r2_arousal:.4f}\")\n",
    "    \n",
    "    # Return evaluation results as a dictionary\n",
    "    eval_results = {\n",
    "        'rmse_valence': rmse_valence,\n",
    "        'rmse_arousal': rmse_arousal,\n",
    "        'normalized_rmse_valence': normalized_rmse_valence,\n",
    "        'normalized_rmse_arousal': normalized_rmse_arousal,\n",
    "        'r2_valence': r2_valence,\n",
    "        'r2_arousal': r2_arousal\n",
    "    }\n",
    "\n",
    "    return eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Á¨¨‰∫åÊ≠•ÔºöÂΩí‰∏ÄÂåñÔºàüí° ËÆ≠ÁªÉÈõÜ fitÔºåval/test Âè™ transformÔºâ\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generic Linear Regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Training function for Linear Regression\n",
    "def train_single_lr(X_train, y_train, max_iter, lr):\n",
    "    model = LinearRegressionModel(X_train.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to train and find the best parameters for Linear Regression\n",
    "def train_lr_torch(X, y_1, y_2, X_val, param_grid=None):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'max_iter': [500, 1000],\n",
    "            'lr': [0.001, 0.01, 0.1]\n",
    "        }\n",
    "\n",
    "    best_val_model = None\n",
    "    best_arou_model = None\n",
    "    best_val_score = float('inf')\n",
    "    best_arou_score = float('inf')\n",
    "    best_max_iter = None\n",
    "    best_lr = None\n",
    "\n",
    "    for max_iter, lr in product(param_grid['max_iter'], param_grid['lr']):\n",
    "        print(f\"üîç Trying config: max_iter={max_iter}, lr={lr}\")\n",
    "\n",
    "        model_val = train_single_lr(X_scaled, y_1, max_iter, lr)\n",
    "        preds_val = model_val(torch.tensor(X_scaled, dtype=torch.float32)).detach().numpy().squeeze()\n",
    "        mse_val = mean_squared_error(y_1, preds_val)\n",
    "\n",
    "        if mse_val < best_val_score:\n",
    "            best_val_score = mse_val\n",
    "            best_val_model = model_val\n",
    "            best_max_iter = max_iter\n",
    "            best_lr = lr\n",
    "\n",
    "        model_arou = train_single_lr(X_scaled, y_2, max_iter, lr)\n",
    "        preds_arou = model_arou(torch.tensor(X_scaled, dtype=torch.float32)).detach().numpy().squeeze()\n",
    "        mse_arou = mean_squared_error(y_2, preds_arou)\n",
    "\n",
    "        if mse_arou < best_arou_score:\n",
    "            best_arou_score = mse_arou\n",
    "            best_arou_model = model_arou\n",
    "\n",
    "    # Áî®ÊúÄ‰ºòÊ®°ÂûãÂú®È™åËØÅÈõÜ‰∏äÈ¢ÑÊµãÂπ∂‰øùÂ≠ò‰∏∫ CSVÔºà‰ªÖÈ™åËØÅÈõÜÔºâ\n",
    "    best_val_model.eval()\n",
    "    best_arou_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "        predictions_val = best_val_model(X_val_tensor).squeeze().numpy()\n",
    "        predictions_arou = best_arou_model(X_val_tensor).squeeze().numpy()\n",
    "\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'pred_valence': predictions_val,\n",
    "        'pred_arousal': predictions_arou\n",
    "    })\n",
    "    df_predictions.to_csv('csv/Multi_predictions_lr_torch.csv', index=False)\n",
    "\n",
    "    print(\"‚úÖ PyTorch Linear Regression training completed and validation predictions saved.\")\n",
    "    return best_val_model, best_arou_model, scaler, best_max_iter, best_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Trying config: max_iter=500, lr=0.001\n",
      "üîç Trying config: max_iter=500, lr=0.01\n",
      "üîç Trying config: max_iter=500, lr=0.1\n",
      "üîç Trying config: max_iter=1000, lr=0.001\n",
      "üîç Trying config: max_iter=1000, lr=0.01\n",
      "üîç Trying config: max_iter=1000, lr=0.1\n",
      "‚úÖ PyTorch Linear Regression training completed and validation predictions saved.\n"
     ]
    }
   ],
   "source": [
    "best_val_model, best_arou_model, scaler, best_max_iter, best_lr = train_lr_torch(\n",
    "    X_train, \n",
    "    y_train_valence, \n",
    "    y_train_arousal, \n",
    "    X_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence: 0.9350\n",
      "RMSE for Arousal: 0.8538\n",
      "Normalized RMSE for Valence: 0.2685\n",
      "Normalized RMSE for Arousal: 0.1678\n",
      "R¬≤ for Valence: 0.1864\n",
      "R¬≤ for Arousal: 0.1938\n"
     ]
    }
   ],
   "source": [
    "results_scaled = evaluate_model(\n",
    "    X_val=X_val,\n",
    "    y_1_validation=y_val_valence,\n",
    "    y_2_validation=y_val_arousal,\n",
    "    model_predictions_file='csv/Multi_predictions_lr_torch.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test RMSE (Valence): 0.9208\n",
      "üìä Test RMSE (Arousal): 0.8567\n",
      "üìà Test R¬≤ (Valence): 0.2357\n",
      "üìà Test R¬≤ (Arousal): 0.1902\n",
      "üßÆ Normalized RMSE (Valence): 0.2492\n",
      "üßÆ Normalized RMSE (Arousal): 0.1683\n"
     ]
    }
   ],
   "source": [
    "# 1. Normalize the test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# 2. Model predictions\n",
    "best_val_model.eval()\n",
    "best_arou_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_val_test = best_val_model(X_test_tensor).squeeze().numpy()\n",
    "    pred_arou_test = best_arou_model(X_test_tensor).squeeze().numpy()\n",
    "\n",
    "# 3. Compute evaluation metrics\n",
    "rmse_val = mean_squared_error(y_test_valence, pred_val_test, squared=False)\n",
    "rmse_arou = mean_squared_error(y_test_arousal, pred_arou_test, squared=False)\n",
    "r2_val = r2_score(y_test_valence, pred_val_test)\n",
    "r2_arou = r2_score(y_test_arousal, pred_arou_test)\n",
    "\n",
    "# Compute label ranges (valence and arousal)\n",
    "valence_range = max(y_test_valence) - min(y_test_valence)\n",
    "arousal_range = max(y_test_arousal) - min(y_test_arousal)\n",
    "\n",
    "# Compute Normalized RMSE\n",
    "nrmse_val = rmse_val / valence_range if valence_range > 0 else None\n",
    "nrmse_arou = rmse_arou / arousal_range if arousal_range > 0 else None\n",
    "\n",
    "# 4. Print results\n",
    "print(f\"üìä Test RMSE (Valence): {rmse_val:.4f}\")\n",
    "print(f\"üìä Test RMSE (Arousal): {rmse_arou:.4f}\")\n",
    "print(f\"üìà Test R¬≤ (Valence): {r2_val:.4f}\")\n",
    "print(f\"üìà Test R¬≤ (Arousal): {r2_arou:.4f}\")\n",
    "print(f\"üßÆ Normalized RMSE (Valence): {nrmse_val:.4f}\")\n",
    "print(f\"üßÆ Normalized RMSE (Arousal): {nrmse_arou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ne'wnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test Set Evaluation (PyTorch Linear Regression)\n",
      "RMSE (Valence): 0.9209\n",
      "RMSE (Arousal): 0.8568\n",
      "R¬≤ (Valence): 0.2356\n",
      "R¬≤ (Arousal): 0.1900\n",
      "Normalized RMSE (Valence): 0.2492\n",
      "Normalized RMSE (Arousal): 0.1684\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è ‰ΩøÁî®ÊúÄ‰ºòË∂ÖÂèÇÊï∞ÈáçÊñ∞ËÆ≠ÁªÉÊúÄÁªàÊ®°ÂûãÔºàLinear RegressionÔºâ\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_val_final = train_single_lr(X_train_scaled, y_train_valence, best_max_iter, best_lr)\n",
    "model_arou_final = train_single_lr(X_train_scaled, y_train_arousal, best_max_iter, best_lr)\n",
    "\n",
    "# üîÆ Âú®ÊµãËØïÈõÜ‰∏äÈ¢ÑÊµã\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "pred_val = model_val_final(X_test_tensor).detach().numpy().squeeze()\n",
    "pred_arou = model_arou_final(X_test_tensor).detach().numpy().squeeze()\n",
    "\n",
    "# üìè ËÆ°ÁÆóËØÑ‰º∞ÊåáÊ†á\n",
    "rmse_val = mean_squared_error(y_test_valence, pred_val, squared=False)\n",
    "rmse_arou = mean_squared_error(y_test_arousal, pred_arou, squared=False)\n",
    "\n",
    "r2_val = r2_score(y_test_valence, pred_val)\n",
    "r2_arou = r2_score(y_test_arousal, pred_arou)\n",
    "\n",
    "val_range = np.max(y_test_valence) - np.min(y_test_valence)\n",
    "arou_range = np.max(y_test_arousal) - np.min(y_test_arousal)\n",
    "nrmse_val = rmse_val / val_range\n",
    "nrmse_arou = rmse_arou / arou_range\n",
    "\n",
    "# üåü ÊâìÂç∞ÁªìÊûú\n",
    "print(\"üìä Test Set Evaluation (PyTorch Linear Regression)\")\n",
    "print(f\"RMSE (Valence): {rmse_val:.4f}\")\n",
    "print(f\"RMSE (Arousal): {rmse_arou:.4f}\")\n",
    "print(f\"R¬≤ (Valence): {r2_val:.4f}\")\n",
    "print(f\"R¬≤ (Arousal): {r2_arou:.4f}\")\n",
    "print(f\"Normalized RMSE (Valence): {nrmse_val:.4f}\")\n",
    "print(f\"Normalized RMSE (Arousal): {nrmse_arou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test RMSE (Valence): 0.9598\n",
      "üìä Test RMSE (Arousal): 0.8799\n",
      "üìà Test R¬≤ (Valence): 0.1697\n",
      "üìà Test R¬≤ (Arousal): 0.1457\n",
      "üßÆ Normalized RMSE (Valence): 0.2598\n",
      "üßÆ Normalized RMSE (Arousal): 0.1729\n"
     ]
    }
   ],
   "source": [
    "# 1. Normalize the test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# 2. Model predictions\n",
    "best_model_val.eval()\n",
    "best_model_arou.eval()\n",
    "with torch.no_grad():\n",
    "    pred_val_test = best_model_val(X_test_tensor).squeeze().numpy()\n",
    "    pred_arou_test = best_model_arou(X_test_tensor).squeeze().numpy()\n",
    "\n",
    "# 3. Compute evaluation metrics\n",
    "rmse_val = mean_squared_error(y_test_valence, pred_val_test, squared=False)\n",
    "rmse_arou = mean_squared_error(y_test_arousal, pred_arou_test, squared=False)\n",
    "r2_val = r2_score(y_test_valence, pred_val_test)\n",
    "r2_arou = r2_score(y_test_arousal, pred_arou_test)\n",
    "\n",
    "# Compute label ranges (valence and arousal)\n",
    "valence_range = max(y_test_valence) - min(y_test_valence)\n",
    "arousal_range = max(y_test_arousal) - min(y_test_arousal)\n",
    "\n",
    "# Compute Normalized RMSE\n",
    "nrmse_val = rmse_val / valence_range if valence_range > 0 else None\n",
    "nrmse_arou = rmse_arou / arousal_range if arousal_range > 0 else None\n",
    "\n",
    "# 4. Print results\n",
    "print(f\"üìä Test RMSE (Valence): {rmse_val:.4f}\")\n",
    "print(f\"üìä Test RMSE (Arousal): {rmse_arou:.4f}\")\n",
    "print(f\"üìà Test R¬≤ (Valence): {r2_val:.4f}\")\n",
    "print(f\"üìà Test R¬≤ (Arousal): {r2_arou:.4f}\")\n",
    "print(f\"üßÆ Normalized RMSE (Valence): {nrmse_val:.4f}\")\n",
    "print(f\"üßÆ Normalized RMSE (Arousal): {nrmse_arou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generic MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        for h in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(in_features, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_features = h\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for one model\n",
    "def train_single_mlp(X_train, y_train, hidden_layers, max_iter):\n",
    "    model = MLP(X_train.shape[1], hidden_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_torch(X, y_1, y_2, X_val, param_grid=None):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(5,), (10,), (15,), (5,5), (10,10), (15,15)],\n",
    "            'max_iter': [500, 1000]\n",
    "        }\n",
    "\n",
    "    best_val_model = None\n",
    "    best_arou_model = None\n",
    "    best_val_score = float('inf')\n",
    "    best_arou_score = float('inf')\n",
    "    best_hidden_layers = None\n",
    "    best_max_iter = None\n",
    "\n",
    "    for hidden_layers, max_iter in product(param_grid['hidden_layer_sizes'], param_grid['max_iter']):\n",
    "        print(f\"üîç Trying config: hidden_layers={hidden_layers}, max_iter={max_iter}\")\n",
    "\n",
    "        model_val = train_single_mlp(X_scaled, y_1, hidden_layers, max_iter)\n",
    "        preds_val = model_val(torch.tensor(X_scaled, dtype=torch.float32)).detach().numpy().squeeze()\n",
    "        mse_val = mean_squared_error(y_1, preds_val)\n",
    "\n",
    "        if mse_val < best_val_score:\n",
    "            best_val_score = mse_val\n",
    "            best_val_model = model_val\n",
    "            best_hidden_layers = hidden_layers\n",
    "            best_max_iter = max_iter\n",
    "\n",
    "        model_arou = train_single_mlp(X_scaled, y_2, hidden_layers, max_iter)\n",
    "        preds_arou = model_arou(torch.tensor(X_scaled, dtype=torch.float32)).detach().numpy().squeeze()\n",
    "        mse_arou = mean_squared_error(y_2, preds_arou)\n",
    "\n",
    "        if mse_arou < best_arou_score:\n",
    "            best_arou_score = mse_arou\n",
    "            best_arou_model = model_arou\n",
    "\n",
    "    # Áî®ÊúÄ‰ºòÊ®°ÂûãÂú®È™åËØÅÈõÜ‰∏äÈ¢ÑÊµãÂπ∂‰øùÂ≠ò‰∏∫ CSVÔºà‰ªÖÈ™åËØÅÈõÜÔºâ\n",
    "    best_val_model.eval()\n",
    "    best_arou_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "        predictions_val = best_val_model(X_val_tensor).squeeze().numpy()\n",
    "        predictions_arou = best_arou_model(X_val_tensor).squeeze().numpy()\n",
    "\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'pred_valence': predictions_val,\n",
    "        'pred_arousal': predictions_arou\n",
    "    })\n",
    "    df_predictions.to_csv('csv/Multi_predictions_mlp_torch.csv', index=False)\n",
    "\n",
    "    print(\"‚úÖ PyTorch MLP training completed and validation predictions saved.\")\n",
    "    return best_val_model, best_arou_model, scaler, best_hidden_layers, best_max_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Trying config: hidden_layers=(5,), max_iter=500\n",
      "üîç Trying config: hidden_layers=(5,), max_iter=1000\n",
      "üîç Trying config: hidden_layers=(10,), max_iter=500\n",
      "üîç Trying config: hidden_layers=(10,), max_iter=1000\n",
      "üîç Trying config: hidden_layers=(15,), max_iter=500\n",
      "üîç Trying config: hidden_layers=(15,), max_iter=1000\n",
      "üîç Trying config: hidden_layers=(5, 5), max_iter=500\n",
      "üîç Trying config: hidden_layers=(5, 5), max_iter=1000\n",
      "üîç Trying config: hidden_layers=(10, 10), max_iter=500\n",
      "üîç Trying config: hidden_layers=(10, 10), max_iter=1000\n",
      "üîç Trying config: hidden_layers=(15, 15), max_iter=500\n",
      "üîç Trying config: hidden_layers=(15, 15), max_iter=1000\n",
      "‚úÖ PyTorch MLP training completed and validation predictions saved.\n"
     ]
    }
   ],
   "source": [
    "best_val_model, best_arou_model, scaler, best_layers, best_iter = train_mlp_torch(\n",
    "    X_train, y_train_valence, y_train_arousal, X_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence: 0.9401\n",
      "RMSE for Arousal: 0.8455\n",
      "Normalized RMSE for Valence: 0.2700\n",
      "Normalized RMSE for Arousal: 0.1662\n",
      "R¬≤ for Valence: 0.1774\n",
      "R¬≤ for Arousal: 0.2093\n"
     ]
    }
   ],
   "source": [
    "results_scaled = evaluate_model(\n",
    "    X_val=X_val_scaled,\n",
    "    y_1_validation=y_val_valence,\n",
    "    y_2_validation=y_val_arousal,\n",
    "    model_predictions_file='csv/Multi_predictions_mlp_torch.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Trying config: hidden_layers=(5,), max_iter=500\n",
      "üîç Trying config: hidden_layers=(5,), max_iter=1000\n",
      "üîç Trying config: hidden_layers=(10,), max_iter=500\n",
      "üîç Trying config: hidden_layers=(10,), max_iter=1000\n",
      "üîç Trying config: hidden_layers=(15,), max_iter=500\n",
      "üîç Trying config: hidden_layers=(15,), max_iter=1000\n",
      "üîç Trying config: hidden_layers=(5, 5), max_iter=500\n",
      "üîç Trying config: hidden_layers=(5, 5), max_iter=1000\n",
      "üîç Trying config: hidden_layers=(10, 10), max_iter=500\n",
      "üîç Trying config: hidden_layers=(10, 10), max_iter=1000\n",
      "üîç Trying config: hidden_layers=(15, 15), max_iter=500\n",
      "üîç Trying config: hidden_layers=(15, 15), max_iter=1000\n",
      "‚úÖ PyTorch MLP training completed and validation predictions saved.\n",
      "üìä Test Set Evaluation (PyTorch MLP)\n",
      "RMSE (Valence): 0.9213\n",
      "RMSE (Arousal): 0.8480\n",
      "R¬≤ (Valence): 0.2349\n",
      "R¬≤ (Arousal): 0.2065\n",
      "Normalized RMSE (Valence): 0.2494\n",
      "Normalized RMSE (Arousal): 0.1666\n"
     ]
    }
   ],
   "source": [
    "# ÂÅáËÆæ‰Ω†ËÆ≠ÁªÉÂÆå‰∫Ü\n",
    "best_val_model, best_arou_model, scaler, best_layers, best_iter = train_mlp_torch(X_train, y_train_valence, y_train_arousal, X_val)\n",
    "\n",
    "# Áî®ËÆ≠ÁªÉÈõÜÈáçÊñ∞ËÆ≠ÁªÉÊúÄÁªàÊ®°ÂûãÔºàPyTorch ÁâàÊú¨Ôºâ\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_val_final = train_single_mlp(X_train_scaled, y_train_valence, best_layers, best_iter)\n",
    "model_arou_final = train_single_mlp(X_train_scaled, y_train_arousal, best_layers, best_iter)\n",
    "\n",
    "# ÊµãËØïÈõÜÈ¢ÑÊµã\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "pred_val = model_val_final(X_test_tensor).detach().numpy().squeeze()\n",
    "pred_arou = model_arou_final(X_test_tensor).detach().numpy().squeeze()\n",
    "\n",
    "# ËÆ°ÁÆóËØÑ‰º∞ÊåáÊ†á\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "rmse_val = mean_squared_error(y_test_valence, pred_val, squared=False)\n",
    "rmse_arou = mean_squared_error(y_test_arousal, pred_arou, squared=False)\n",
    "\n",
    "r2_val = r2_score(y_test_valence, pred_val)\n",
    "r2_arou = r2_score(y_test_arousal, pred_arou)\n",
    "\n",
    "# Normalized RMSE\n",
    "val_range = np.max(y_test_valence) - np.min(y_test_valence)\n",
    "arou_range = np.max(y_test_arousal) - np.min(y_test_arousal)\n",
    "nrmse_val = rmse_val / val_range\n",
    "nrmse_arou = rmse_arou / arou_range\n",
    "\n",
    "# ÊâìÂç∞ÁªìÊûú\n",
    "print(\"üìä Test Set Evaluation (PyTorch MLP)\")\n",
    "print(f\"RMSE (Valence): {rmse_val:.4f}\")\n",
    "print(f\"RMSE (Arousal): {rmse_arou:.4f}\")\n",
    "print(f\"R¬≤ (Valence): {r2_val:.4f}\")\n",
    "print(f\"R¬≤ (Arousal): {r2_arou:.4f}\")\n",
    "print(f\"Normalized RMSE (Valence): {nrmse_val:.4f}\")\n",
    "print(f\"Normalized RMSE (Arousal): {nrmse_arou:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
