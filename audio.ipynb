{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error #add rmse\n",
    "from data import merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and drop VADER columns\n",
    "df_train = pd.read_csv('data/merged/merged_cleaned_sentiment_train.csv').drop(['pos','neg','neu', 'compound'], axis = 1)\n",
    "df_val = pd.read_csv('data/merged/merged_cleaned_sentiment_validation.csv').drop(['pos','neg','neu', 'compound'], axis = 1)\n",
    "df_test = pd.read_csv('data/merged/merged_cleaned_sentiment_test.csv').drop(['pos','neg','neu', 'compound'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save audio features\n",
    "# 5 audio features + 2 real target\n",
    "df_train = df_train[['danceability', 'energy', 'instrumentalness', 'valence','mode', 'y_valence', 'y_arousal']]\n",
    "df_val = df_val[['danceability', 'energy', 'instrumentalness', 'valence','mode', 'y_valence', 'y_arousal']]\n",
    "df_test = df_test[['danceability', 'energy', 'instrumentalness', 'valence','mode','y_valence', 'y_arousal']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values from the training, validation, and test datasets\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['danceability', 'energy', 'instrumentalness', 'valence', 'mode',\n",
       "       'y_valence', 'y_arousal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output colums\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training set\n",
    "# X_train: Features for training set, excluding the target variables 'y_valence' and 'y_arousal'\n",
    "X_train = df_train.drop(['y_valence', 'y_arousal'], axis=1).values\n",
    "# y_train_valence: Target variable 'y_valence' for training set\n",
    "y_train_valence = df_train.y_valence.values \n",
    "# y_train_arousal: Target variable 'y_arousal' for training set\n",
    "y_train_arousal = df_train.y_arousal.values\n",
    "    \n",
    "# Validation set\n",
    "# X_val: Features for validation set, excluding the target variables 'y_valence' and 'y_arousal'\n",
    "X_val = df_val.drop(['y_valence', 'y_arousal'], axis=1).values\n",
    "# y_val_valence: Target variable 'y_valence' for validation set\n",
    "y_val_valence = df_val.y_valence.values \n",
    "# y_val_arousal: Target variable 'y_arousal' for validation set\n",
    "y_val_arousal = df_val.y_arousal.values \n",
    "\n",
    "# Test set\n",
    "# X_test: Features for test set, excluding the target variables 'y_valence' and 'y_arousal'\n",
    "X_test = df_test.drop(['y_valence', 'y_arousal'], axis=1).values\n",
    "# y_test_valence: Target variable 'y_valence' for test set\n",
    "y_test_valence = df_test.y_valence.values \n",
    "# y_test_arousal: Target variable 'y_arousal' for test set\n",
    "y_test_arousal = df_test.y_arousal.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(X_val, y_1_validation, y_2_validation, model_predictions_file='predictions.csv'):\n",
    "    \"\"\"Evaluate the trained model using different evaluation criteria, including Normalized RMSE\"\"\"\n",
    "    \n",
    "    # Load the saved predictions from the CSV file\n",
    "    df_predictions = pd.read_csv(model_predictions_file)\n",
    "    \n",
    "    # Get the true values from validation data\n",
    "    true_valence = y_1_validation\n",
    "    true_arousal = y_2_validation\n",
    "\n",
    "    # Ensure predictions are in the original range (if necessary)\n",
    "    # If predictions are standardized, use the inverse_transform of your scaler before proceeding.\n",
    "    # Example: df_predictions['pred_valence'] = scaler.inverse_transform(df_predictions[['pred_valence']])\n",
    "\n",
    "    # Compute RMSE\n",
    "    rmse_valence = mean_squared_error(true_valence, df_predictions['pred_valence'], squared=False)\n",
    "    rmse_arousal = mean_squared_error(true_arousal, df_predictions['pred_arousal'], squared=False)\n",
    "\n",
    "    # Compute Normalized RMSE\n",
    "    valence_range = max(true_valence) - min(true_valence)\n",
    "    arousal_range = max(true_arousal) - min(true_arousal)\n",
    "\n",
    "    normalized_rmse_valence = rmse_valence / valence_range if valence_range > 0 else None\n",
    "    normalized_rmse_arousal = rmse_arousal / arousal_range if arousal_range > 0 else None\n",
    "\n",
    "    # Compute RÂ²\n",
    "    r2_valence = r2_score(true_valence, df_predictions['pred_valence'])\n",
    "    r2_arousal = r2_score(true_arousal, df_predictions['pred_arousal'])\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"RMSE for Valence: {rmse_valence:.4f}\")\n",
    "    print(f\"RMSE for Arousal: {rmse_arousal:.4f}\")\n",
    "    print(f\"Normalized RMSE for Valence: {normalized_rmse_valence:.4f}\" if normalized_rmse_valence is not None else \"Valence range is zero, cannot compute NRMSE.\")\n",
    "    print(f\"Normalized RMSE for Arousal: {normalized_rmse_arousal:.4f}\" if normalized_rmse_arousal is not None else \"Arousal range is zero, cannot compute NRMSE.\")\n",
    "    print(f\"RÂ² for Valence: {r2_valence:.4f}\")\n",
    "    print(f\"RÂ² for Arousal: {r2_arousal:.4f}\")\n",
    "    \n",
    "    # Return evaluation results as a dictionary\n",
    "    eval_results = {\n",
    "        'rmse_valence': rmse_valence,\n",
    "        'rmse_arousal': rmse_arousal,\n",
    "        'normalized_rmse_valence': normalized_rmse_valence,\n",
    "        'normalized_rmse_arousal': normalized_rmse_arousal,\n",
    "        'r2_valence': r2_valence,\n",
    "        'r2_arousal': r2_arousal\n",
    "    }\n",
    "\n",
    "    return eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_on_test(X_test, y_test_valence, y_test_arousal, model_valence, model_arousal,scaler_X=None):\n",
    "    \"\"\"\n",
    "    åœ¨åŽŸå§‹å°ºåº¦ä¸Šè¯„ä¼°æ¨¡åž‹æ€§èƒ½ï¼šè¾“å‡º RMSEã€Normalized RMSE å’Œ RÂ²ã€‚\n",
    "    \"\"\"\n",
    "    # 1. å½’ä¸€åŒ–æµ‹è¯•é›†ç‰¹å¾ï¼ˆå¦‚æžœæä¾›äº† scalerï¼‰\n",
    "    if scaler_X is not None:\n",
    "     X_test = scaler_X.transform(X_test)\n",
    "    # 1. æ¨¡åž‹é¢„æµ‹\n",
    "    pred_val = model_valence.predict(X_test)\n",
    "    pred_arou = model_arousal.predict(X_test)\n",
    "\n",
    "    # 2. RMSE\n",
    "    rmse_val = mean_squared_error(y_test_valence, pred_val, squared=False)\n",
    "    rmse_arou = mean_squared_error(y_test_arousal, pred_arou, squared=False)\n",
    "\n",
    "    # 3. Normalized RMSE\n",
    "    valence_range = np.max(y_test_valence) - np.min(y_test_valence)\n",
    "    arousal_range = np.max(y_test_arousal) - np.min(y_test_arousal)\n",
    "\n",
    "    nrmse_val = rmse_val / valence_range if valence_range > 0 else None\n",
    "    nrmse_arou = rmse_arou / arousal_range if arousal_range > 0 else None\n",
    "\n",
    "    # 4. RÂ²\n",
    "    r2_val = r2_score(y_test_valence, pred_val)\n",
    "    r2_arou = r2_score(y_test_arousal, pred_arou)\n",
    "\n",
    "    # 5. è¾“å‡ºç»“æžœ\n",
    "    print(\"ðŸ“Š [Test Set Evaluation]\")\n",
    "    print(f\"RMSE (Valence): {rmse_val:.4f}\")\n",
    "    print(f\"RMSE (Arousal): {rmse_arou:.4f}\")\n",
    "    print(f\"Normalized RMSE (Valence): {nrmse_val:.4f}\" if nrmse_val is not None else \"Valence range is zero, cannot compute NRMSE.\")\n",
    "    print(f\"Normalized RMSE (Arousal): {nrmse_arou:.4f}\" if nrmse_arou is not None else \"Arousal range is zero, cannot compute NRMSE.\")\n",
    "    print(f\"RÂ² (Valence): {r2_val:.4f}\")\n",
    "    print(f\"RÂ² (Arousal): {r2_arou:.4f}\")\n",
    "\n",
    "    # 6. è¿”å›žå¯é€‰ç»“æžœå­—å…¸\n",
    "    return {\n",
    "        'rmse_valence': rmse_val,\n",
    "        'rmse_arousal': rmse_arou,\n",
    "        'normalized_rmse_valence': nrmse_val,\n",
    "        'normalized_rmse_arousal': nrmse_arou,\n",
    "        'r2_valence': r2_val,\n",
    "        'r2_arousal': r2_arou\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization to True y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# åˆå§‹åŒ– scaler\n",
    "scaler_valence = MinMaxScaler()\n",
    "scaler_arousal = MinMaxScaler()\n",
    "\n",
    "# å½’ä¸€åŒ–è®­ç»ƒé›†ç›®æ ‡å˜é‡\n",
    "y_train_valence_scaled = scaler_valence.fit_transform(y_train_valence.reshape(-1, 1)).flatten()\n",
    "y_train_arousal_scaled = scaler_arousal.fit_transform(y_train_arousal.reshape(-1, 1)).flatten()\n",
    "\n",
    "# éªŒè¯é›†ä½¿ç”¨ç›¸åŒçš„ scaler è½¬æ¢ï¼ˆä¸èƒ½é‡æ–° fitï¼ï¼‰\n",
    "y_val_valence_scaled = scaler_valence.transform(y_val_valence.reshape(-1, 1)).flatten()\n",
    "y_val_arousal_scaled = scaler_arousal.transform(y_val_arousal.reshape(-1, 1)).flatten()\n",
    "\n",
    "# æµ‹è¯•é›†åŒæ ·å¦‚æ­¤ï¼ˆå¦‚æžœä½ ç”¨å®ƒï¼‰\n",
    "y_test_valence_scaled = scaler_valence.transform(y_test_valence.reshape(-1, 1)).flatten()\n",
    "y_test_arousal_scaled = scaler_arousal.transform(y_test_arousal.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1 Without Nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_regression(X, y_1, y_2, X_val, param_grid=None):\n",
    "    \"\"\" Train the regression model with GridSearchCV \"\"\"\n",
    "    \n",
    "    # Default parameters for grid search\n",
    "    if param_grid is None:\n",
    "        param_grid = {'fit_intercept': [True, False], 'positive': [True, False]}\n",
    "    \n",
    "    # Initialize models for both targets\n",
    "    lr_val = LinearRegression()\n",
    "    lr_arou = LinearRegression()\n",
    "    \n",
    "    # Grid search for both models\n",
    "    clf_vale = GridSearchCV(lr_val, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "    \n",
    "    clf_arou = GridSearchCV(lr_arou, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Fit models to training data\n",
    "    clf_vale.fit(X, y_1)\n",
    "    clf_arou.fit(X, y_2)\n",
    "    \n",
    "    # Print best results on training data\n",
    "    print()\n",
    "    print(f\"Best parameter for Valence (CV score={clf_vale.best_score_:.3f}):\")\n",
    "    print(clf_vale.best_params_)\n",
    "    \n",
    "    print()\n",
    "    print(f\"Best parameter for Arousal (CV score={clf_arou.best_score_:.3f}):\")\n",
    "    print(clf_arou.best_params_)\n",
    "\n",
    "    # Initialize models with best parameters\n",
    "    lr_val_top = LinearRegression(fit_intercept=clf_vale.best_params_['fit_intercept'],  \n",
    "                                  positive=clf_vale.best_params_['positive'])\n",
    "    lr_arou_top = LinearRegression(fit_intercept=clf_arou.best_params_['fit_intercept'],  \n",
    "                                   positive=clf_arou.best_params_['positive'])\n",
    "\n",
    "    # Fit optimal models to the entire training data\n",
    "    lr_val_top.fit(X, y_1)\n",
    "    lr_arou_top.fit(X, y_2)\n",
    "\n",
    "    # Predict for validation data\n",
    "    predictions_val = lr_val_top.predict(X_val)  # Predictions for Valence\n",
    "    predictions_arou = lr_arou_top.predict(X_val)  # Predictions for Arousal\n",
    "\n",
    "    # Save predictions for validation data to CSV\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'pred_valence': predictions_val,  # Valence predictions\n",
    "        'pred_arousal': predictions_arou  # Arousal predictions\n",
    "    })\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    df_predictions.to_csv('AUdio_prediction.csv', index=False)\n",
    "\n",
    "    print(\"âœ… Training completed and predictions saved!\")\n",
    "    return lr_val_top, lr_arou_top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2 Nor+track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def train_regression_scaled_y(X, y_1_scaled, y_2_scaled, X_val, track_ids, param_grid=None, output_file='AUdio_prediction.csv'):\n",
    "    \"\"\"\n",
    "    Train regression model using pre-scaled y values (Valence and Arousal).\n",
    "    Predictions are not inverse-transformed and are saved as scaled.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Set default parameters if none provided\n",
    "    if param_grid is None:\n",
    "        param_grid = {'fit_intercept': [True, False], 'positive': [True, False]}\n",
    "\n",
    "    # Step 2: Initialize base models\n",
    "    lr_val = LinearRegression()\n",
    "    lr_arou = LinearRegression()\n",
    "\n",
    "    # Step 3: Grid search\n",
    "    clf_vale = GridSearchCV(lr_val, param_grid, scoring='neg_mean_squared_error',\n",
    "                            verbose=1, n_jobs=-1, return_train_score=True)\n",
    "    clf_arou = GridSearchCV(lr_arou, param_grid, scoring='neg_mean_squared_error',\n",
    "                            verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "    # Step 4: Fit models on scaled y\n",
    "    clf_vale.fit(X, y_1_scaled)\n",
    "    clf_arou.fit(X, y_2_scaled)\n",
    "\n",
    "    print(f\"\\nBest Valence Params (CV Score: {clf_vale.best_score_:.3f}): {clf_vale.best_params_}\")\n",
    "    print(f\"Best Arousal Params (CV Score: {clf_arou.best_score_:.3f}): {clf_arou.best_params_}\")\n",
    "\n",
    "    # Step 5: Refit best models\n",
    "    best_val = LinearRegression(**clf_vale.best_params_)\n",
    "    best_arou = LinearRegression(**clf_arou.best_params_)\n",
    "\n",
    "    best_val.fit(X, y_1_scaled)\n",
    "    best_arou.fit(X, y_2_scaled)\n",
    "\n",
    "    # Step 6: Predict on validation set (still scaled)\n",
    "    pred_val_scaled = best_val.predict(X_val)\n",
    "    pred_arou_scaled = best_arou.predict(X_val)\n",
    "\n",
    "    # Step 7: Save predictions to CSV\n",
    "    if len(track_ids) != len(pred_val_scaled):\n",
    "        raise ValueError(\"Mismatch: The number of track IDs does not match the number of predictions!\")\n",
    "\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'track_id': track_ids,\n",
    "        'pred_valence': pred_val_scaled,\n",
    "        'pred_arousal': pred_arou_scaled\n",
    "    })\n",
    "\n",
    "    df_predictions.to_csv(output_file, index=False)\n",
    "    print(f\"âœ… Scaled predictions saved to {output_file}!\")\n",
    "\n",
    "    return best_val, best_arou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Best Valence Params (CV Score: -0.072): {'fit_intercept': True, 'positive': False}\n",
      "Best Arousal Params (CV Score: -0.030): {'fit_intercept': True, 'positive': False}\n",
      "âœ… Scaled predictions saved to AUdio_prediction_scaled.csv!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr_val_model, lr_arou_model = train_regression_scaled_y(\n",
    "    X_train,\n",
    "    y_train_valence_scaled,\n",
    "    y_train_arousal_scaled,\n",
    "    X_val,\n",
    "    track_ids=df_validation['trackname'].values,\n",
    "    output_file='AUdio_prediction_scaled.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Best parameter for Valence (CV score=-0.982):\n",
      "{'fit_intercept': True, 'positive': False}\n",
      "\n",
      "Best parameter for Arousal (CV score=-0.774):\n",
      "{'fit_intercept': True, 'positive': False}\n",
      "âœ… Training completed and predictions saved with Track IDs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearRegression(), LinearRegression())"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_csv('data/merged/merged_cleaned_sentiment_validation.csv')\n",
    "\n",
    "# å‡è®¾éªŒè¯é›†æ˜¯ä»ŽåŽŸå§‹æ•°æ®é›†ä¸­é€‰å‡ºçš„\n",
    "df_validation = df.iloc[:len(X_val)]  # åªä¿ç•™éªŒè¯é›†éƒ¨åˆ†\n",
    "\n",
    "# æå–éªŒè¯é›†å¯¹åº”çš„ Track ID\n",
    "track_ids = df_validation['trackname'].values  # æˆ–è€… df_validation['track_id'] å–å”¯ä¸€æ ‡è¯†ç¬¦\n",
    "\n",
    "# è®­ç»ƒå›žå½’æ¨¡åž‹ï¼Œå¹¶å­˜å‚¨é¢„æµ‹ç»“æžœ\n",
    "train_regression(X_train, y_train_valence, y_train_arousal, X_val, track_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Best parameter for Valence (CV score=-0.982):\n",
      "{'fit_intercept': True, 'positive': False}\n",
      "\n",
      "Best parameter for Arousal (CV score=-0.774):\n",
      "{'fit_intercept': True, 'positive': False}\n",
      "âœ… Training completed and predictions saved!\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train, y_train, X_validation, y_validation are your data\n",
    "lr_val_model, lr_arou_model = train_regression(X_train, y_train_valence, y_train_arousal, X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence: 0.2614\n",
      "RMSE for Arousal: 0.1670\n",
      "Normalized RMSE for Valence: 0.2774\n",
      "Normalized RMSE for Arousal: 0.1670\n",
      "RÂ² for Valence: 0.1317\n",
      "RÂ² for Arousal: 0.2009\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_val, y_valence_val, y_arousal_val are your validation data\n",
    "eval_results = evaluate_model(X_val, y_val_valence_scaled ,y_val_arousal_scaled,model_predictions_file='AUdio_prediction_scaled.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE for Valence: 0.966\n",
    "RMSE for Arousal: 0.850\n",
    "Normalized RMSE for Valence: 0.277\n",
    "Normalized RMSE for Arousal: 0.167\n",
    "RÂ² for Valence: 0.132\n",
    "RÂ² for Arousal: 0.201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling: -2.14809720439 1.54671444505\n",
      "After scaling: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Before scaling:\", y_train_valence.min(), y_train_valence.max())\n",
    "print(\"After scaling:\", y_train_valence_scaled.min(), y_train_valence_scaled.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š [Test Set Evaluation]\n",
      "RMSE (Valence): 0.9594\n",
      "RMSE (Arousal): 0.8554\n",
      "Normalized RMSE (Valence): 0.2597\n",
      "Normalized RMSE (Arousal): 0.1681\n",
      "RÂ² (Valence): 0.1703\n",
      "RÂ² (Arousal): 0.1928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# è®¡ç®— MSE\n",
    "mse_valence = mean_squared_error(y_test_valence, lr_val_model.predict(X_test))\n",
    "mse_arousal = mean_squared_error(y_test_arousal, lr_arou_model.predict(X_test))\n",
    "\n",
    "# è®¡ç®— Normalized RMSE\n",
    "valence_range = y_test_valence.max() - y_test_valence.min()\n",
    "arousal_range = y_test_arousal.max() - y_test_arousal.min()\n",
    "\n",
    "nrmse_valence = (mse_valence ** 0.5) / valence_range if valence_range > 0 else None\n",
    "nrmse_arousal = (mse_arousal ** 0.5) / arousal_range if arousal_range > 0 else None\n",
    "\n",
    "print(f\"MSE for Valence: {mse_valence:.4f}\")\n",
    "print(f\"MSE for Arousal: {mse_arousal:.4f}\")\n",
    "print(f\"Normalized RMSE for Valence: {nrmse_valence:.4f}\" if nrmse_valence is not None else \"Valence range is zero, cannot compute NRMSE.\")\n",
    "print(f\"Normalized RMSE for Arousal: {nrmse_arousal:.4f}\" if nrmse_arousal is not None else \"Arousal range is zero, cannot compute NRMSE.\")\n",
    "\n",
    "# è¯„ä¼°æµ‹è¯•é›†ä¸Šçš„æ¨¡åž‹æ€§èƒ½\n",
    "results = evaluate_on_test(\n",
    "    X_test,\n",
    "    y_test_valence,\n",
    "    y_test_arousal,\n",
    "    lr_val_model,\n",
    "    lr_arou_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def train_mlp(X, y_1, y_2, X_val, param_grid=None):\n",
    "    \"\"\" Train the MLP model with GridSearchCV and save the predictions \"\"\"\n",
    "\n",
    "    # Normalize the features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Default parameters for grid search\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(5,), (10,), (15,), (5,5), (10,10), (15,15), (5,5,5), (10,10,10), (15,15,15)], \n",
    "            'max_iter': [500, 1000, 2000, 2500]\n",
    "        }\n",
    "\n",
    "    # Initialize models for both targets\n",
    "    mlp_val = MLPRegressor(random_state=2)\n",
    "    mlp_arou = MLPRegressor(random_state=2)\n",
    "    \n",
    "    # Grid search for Valence\n",
    "    clf_vale = GridSearchCV(mlp_val, param_grid, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1, return_train_score=True)\n",
    "    clf_vale.fit(X, y_1)\n",
    "    print(f\"Best parameter for Valence (CV score={-clf_vale.best_score_:.3f}): {clf_vale.best_params_}\")\n",
    "    \n",
    "    # Initialize model with best parameters and fit\n",
    "    mlp_val_top = MLPRegressor(hidden_layer_sizes=clf_vale.best_params_['hidden_layer_sizes'],\n",
    "                               max_iter=clf_vale.best_params_['max_iter'],\n",
    "                               random_state=2)\n",
    "    mlp_val_top.fit(X, y_1)\n",
    "\n",
    "    # Grid search for Arousal\n",
    "    clf_arou = GridSearchCV(mlp_arou, param_grid, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1, return_train_score=True)\n",
    "    clf_arou.fit(X, y_2)\n",
    "    print(f\"Best parameter for Arousal (CV score={-clf_arou.best_score_:.3f}): {clf_arou.best_params_}\")\n",
    "    \n",
    "    # Initialize model with best parameters and fit\n",
    "    mlp_arou_top = MLPRegressor(hidden_layer_sizes=clf_arou.best_params_['hidden_layer_sizes'],\n",
    "                                max_iter=clf_arou.best_params_['max_iter'],\n",
    "                                random_state=2)\n",
    "    mlp_arou_top.fit(X, y_2)\n",
    "\n",
    "    # Predict for validation data\n",
    "    predictions_val = mlp_val_top.predict(X_val)  # Predictions for Valence\n",
    "    predictions_arou = mlp_arou_top.predict(X_val)  # Predictions for Arousal\n",
    "\n",
    "    # Save predictions for validation data to CSV\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'pred_valence': predictions_val,  # Valence predictions\n",
    "        'pred_arousal': predictions_arou  # Arousal predictions\n",
    "    })\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    df_predictions.to_csv('AUdio_predictions_mlp.csv', index=False)\n",
    "\n",
    "    print(\"âœ… MLP training completed and predictions saved!\")\n",
    "    return mlp_val_top, mlp_arou_top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameter for Valence (CV score=0.968): {'hidden_layer_sizes': (15,), 'max_iter': 500}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameter for Arousal (CV score=0.756): {'hidden_layer_sizes': (10, 10, 10), 'max_iter': 500}\n",
      "âœ… MLP training completed and predictions saved!\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train, y_train_valence, y_train_arousal, X_val are available\n",
    "mlp_val_model, mlp_arou_model = train_mlp(X_train, y_train_valence, y_train_arousal, X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence: 0.9624\n",
      "RMSE for Arousal: 0.8360\n",
      "Normalized RMSE for Valence: 0.2764\n",
      "Normalized RMSE for Arousal: 0.1643\n",
      "RÂ² for Valence: 0.1379\n",
      "RÂ² for Arousal: 0.2270\n"
     ]
    }
   ],
   "source": [
    "eval_results = evaluate_model(X_val, y_val_valence, y_val_arousal, model_predictions_file='AUdio_predictions_mlp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17802320341063427\n",
      "0.20265538858827592\n"
     ]
    }
   ],
   "source": [
    "### Test Set - MLP \n",
    "mlp_val = MLPRegressor(hidden_layer_sizes=mlp_val_model.hidden_layer_sizes, max_iter=mlp_val_model.max_iter, random_state=2)\n",
    "mlp_arou = MLPRegressor(hidden_layer_sizes=mlp_arou_model.hidden_layer_sizes, max_iter=mlp_arou_model.max_iter, random_state=2)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled.shape\n",
    "print(mlp_val.fit(X_train_scaled, y_train_valence).score(X_test_scaled, y_test_valence))\n",
    "print(mlp_arou.fit(X_train_scaled, y_train_arousal).score(X_test_scaled, y_test_arousal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š [Test Set Evaluation]\n",
      "RMSE (Valence): 0.9561\n",
      "RMSE (Arousal): 0.8502\n",
      "Normalized RMSE (Valence): 0.2588\n",
      "Normalized RMSE (Arousal): 0.1671\n",
      "RÂ² (Valence): 0.1760\n",
      "RÂ² (Arousal): 0.2024\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°æµ‹è¯•é›†ä¸Šçš„ MLP æ¨¡åž‹æ€§èƒ½\n",
    "results_mlp = evaluate_on_test(\n",
    "    X_test,\n",
    "    y_test_valence,\n",
    "    y_test_arousal,\n",
    "    mlp_val_model,\n",
    "    mlp_arou_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def train_rf(X, y_1, y_2, X_val):\n",
    "    \"\"\" Train Random Forest model and save predictions for validation data \"\"\"\n",
    "\n",
    "    # Initialize models for Valence and Arousal\n",
    "    rf_val = RandomForestRegressor(random_state=0)\n",
    "    rf_arou = RandomForestRegressor(random_state=0)\n",
    "\n",
    "    # Hyperparameter grid for GridSearchCV\n",
    "    param_grid = { \n",
    "        'n_estimators': [100, 500],\n",
    "        'max_depth': [5, 10, 15]\n",
    "    }\n",
    "\n",
    "    # Grid search for Valence model\n",
    "    clf_vale = GridSearchCV(rf_val, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Grid search for Arousal model\n",
    "    clf_arou = GridSearchCV(rf_arou, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Fit models for Valence and Arousal\n",
    "    clf_vale.fit(X, y_1)\n",
    "    clf_arou.fit(X, y_2)\n",
    "\n",
    "    # Print best parameters found by GridSearchCV\n",
    "    print(f\"Best parameter for Valence (CV score={-clf_vale.best_score_:.3f}):\")\n",
    "    print(clf_vale.best_params_)\n",
    "\n",
    "    print(f\"Best parameter for Arousal (CV score={-clf_arou.best_score_:.3f}):\")\n",
    "    print(clf_arou.best_params_)\n",
    "\n",
    "    # Initialize models with best parameters\n",
    "    rf_val_best = RandomForestRegressor(n_estimators=clf_vale.best_params_['n_estimators'],\n",
    "                                        max_depth=clf_vale.best_params_['max_depth'],\n",
    "                                        random_state=0)\n",
    "    \n",
    "    rf_arou_best = RandomForestRegressor(n_estimators=clf_arou.best_params_['n_estimators'],\n",
    "                                         max_depth=clf_arou.best_params_['max_depth'],\n",
    "                                         random_state=0)\n",
    "\n",
    "    # Fit models with best parameters\n",
    "    rf_val_best.fit(X, y_1)\n",
    "    rf_arou_best.fit(X, y_2)\n",
    "\n",
    "    # Predict for validation data\n",
    "    predictions_val = rf_val_best.predict(X_val)  # Predictions for Valence\n",
    "    predictions_arou = rf_arou_best.predict(X_val)  # Predictions for Arousal\n",
    "\n",
    "    # Save predictions to a DataFrame\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'pred_valence': predictions_val,  # Valence predictions\n",
    "        'pred_arousal': predictions_arou  # Arousal predictions\n",
    "    })\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    df_predictions.to_csv('AUdio_predictions_rf.csv', index=False)\n",
    "\n",
    "    print(\"âœ… Random Forest training completed and predictions saved!\")\n",
    "\n",
    "    return rf_val_best, rf_arou_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameter for Valence (CV score=0.976):\n",
      "{'max_depth': 5, 'n_estimators': 500}\n",
      "Best parameter for Arousal (CV score=0.751):\n",
      "{'max_depth': 5, 'n_estimators': 500}\n",
      "âœ… Random Forest training completed and predictions saved!\n"
     ]
    }
   ],
   "source": [
    "rf_val_model, rf_arou_model = train_rf(X_train, y_train_valence, y_train_arousal, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence: 0.9642\n",
      "RMSE for Arousal: 0.8331\n",
      "Normalized RMSE for Valence: 0.2769\n",
      "Normalized RMSE for Arousal: 0.1637\n",
      "RÂ² for Valence: 0.1346\n",
      "RÂ² for Arousal: 0.2324\n"
     ]
    }
   ],
   "source": [
    "eval_results = evaluate_model(X_val, y_val_valence, y_val_arousal, model_predictions_file='AUdio_predictions_rf.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1710578256745422\n",
      "0.20400503272081394\n"
     ]
    }
   ],
   "source": [
    "#### Test Set - RF\n",
    "rf_val = RandomForestRegressor(n_estimators = 100, \n",
    "                                        max_depth = 5, random_state=0)\n",
    "rf_arou = RandomForestRegressor(n_estimators = 100, \n",
    "                                        max_depth = 5, random_state=0)\n",
    "\n",
    "print(rf_val.fit(X_train, y_train_valence).score(X_test, y_test_valence))\n",
    "print(rf_arou.fit(X_train, y_train_arousal).score(X_test, y_test_arousal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š [Test Set Evaluation]\n",
      "RMSE (Valence): 0.9583\n",
      "RMSE (Arousal): 0.8494\n",
      "Normalized RMSE (Valence): 0.2594\n",
      "Normalized RMSE (Arousal): 0.1669\n",
      "RÂ² (Valence): 0.1723\n",
      "RÂ² (Arousal): 0.2039\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°æµ‹è¯•é›†ä¸Šçš„ Random Forest æ¨¡åž‹æ€§èƒ½\n",
    "results_rf = evaluate_on_test(\n",
    "    X_test,\n",
    "    y_test_valence,\n",
    "    y_test_arousal,\n",
    "    rf_val_model,\n",
    "    rf_arou_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Def Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_svr_linear(X, y_1, y_2, X_val):\n",
    "    \"\"\" Train SVR model (linear kernel) with GridSearch over C, and save predictions for validation data \"\"\"\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Define base model\n",
    "    base_svr = SVR(kernel='linear')\n",
    "\n",
    "    # Hyperparameter grid (only for C)\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 5, 10, 100]\n",
    "    }\n",
    "\n",
    "    # Grid search for Valence model\n",
    "    clf_vale = GridSearchCV(base_svr, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Grid search for Arousal model\n",
    "    clf_arou = GridSearchCV(base_svr, \n",
    "                            param_grid, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    # Fit models\n",
    "    clf_vale.fit(X_scaled, y_1)\n",
    "    clf_arou.fit(X_scaled, y_2)\n",
    "\n",
    "    # Print best parameters\n",
    "    print(f\"Best C for Valence (CV score={-clf_vale.best_score_:.3f}): {clf_vale.best_params_['C']}\")\n",
    "    print(f\"Best C for Arousal (CV score={-clf_arou.best_score_:.3f}): {clf_arou.best_params_['C']}\")\n",
    "\n",
    "    # Use best estimators to predict\n",
    "    svr_val_best = clf_vale.best_estimator_\n",
    "    svr_arou_best = clf_arou.best_estimator_\n",
    "\n",
    "    predictions_val = svr_val_best.predict(X_val_scaled)\n",
    "    predictions_arou = svr_arou_best.predict(X_val_scaled)\n",
    "\n",
    "    # Save predictions\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'pred_valence': predictions_val,\n",
    "        'pred_arousal': predictions_arou\n",
    "    })\n",
    "\n",
    "    df_predictions.to_csv('Audio_predictions_svr_linear_gridC.csv', index=False)\n",
    "\n",
    "    print(\"âœ… SVR (linear kernel with GridSearch on C) training completed and predictions saved!\")\n",
    "\n",
    "    return svr_val_best, svr_arou_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best C for Valence (CV score=1.011): 0.1\n",
      "Best C for Arousal (CV score=0.776): 0.1\n",
      "âœ… SVR (linear kernel with GridSearch on C) training completed and predictions saved!\n"
     ]
    }
   ],
   "source": [
    "svr_val_model, svr_arou_model = train_svr_linear(X_train, y_train_valence, y_train_arousal, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Valence: 0.9784\n",
      "RMSE for Arousal: 0.8504\n",
      "Normalized RMSE for Valence: 0.2810\n",
      "Normalized RMSE for Arousal: 0.1671\n",
      "RÂ² for Valence: 0.1091\n",
      "RÂ² for Arousal: 0.2001\n"
     ]
    }
   ],
   "source": [
    "eval_results = evaluate_model(X_val, y_val_valence, y_val_arousal, model_predictions_file='Audio_predictions_svr_linear_gridC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15832727745087072\n",
      "0.18879866121237865\n"
     ]
    }
   ],
   "source": [
    "#### Test Score - SVR\n",
    "svr_val = SVR(kernel=svr_val_model.kernel, C=svr_val_model.C)\n",
    "svr_arou = SVR(kernel=svr_arou_model.kernel, C=svr_arou_model.C)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(svr_val.fit(X_train, y_train_valence).score(X_test, y_test_valence))\n",
    "print(svr_arou.fit(X_train, y_train_arousal).score(X_test, y_test_arousal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š [Test Set Evaluation]\n",
      "RMSE (Valence): 0.9674\n",
      "RMSE (Arousal): 0.8576\n",
      "Normalized RMSE (Valence): 0.2618\n",
      "Normalized RMSE (Arousal): 0.1685\n",
      "RÂ² (Valence): 0.1565\n",
      "RÂ² (Arousal): 0.1885\n"
     ]
    }
   ],
   "source": [
    "results_svr = evaluate_on_test(\n",
    "    X_test,\n",
    "    y_test_valence,\n",
    "    y_test_arousal,\n",
    "    svr_val_model,\n",
    "    svr_arou_model,\n",
    "    scaler_X=scaler  # âœ… ä¼ å…¥ç”¨äºŽè®­ç»ƒçš„ scaler\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
